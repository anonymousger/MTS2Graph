{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed Nov 11 13:54:41 2020\n",
    "\n",
    "@author: \n",
    "\n",
    "Read a dataset and train a model for classify the data.\n",
    "\"\"\"\n",
    "from Data_Preprocessing import ReadData\n",
    "from ConvNet_Model import ConvNet\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras\n",
    "import sys, getopt\n",
    "from High_Activated_Filters_CNN import HighlyActivated\n",
    "import pandas\n",
    "from itertools import *\n",
    "from  functools import *\n",
    "from Clustering import Clustering\n",
    "from matplotlib import pyplot\n",
    "import tensorflow.keras as keras\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from scipy import stats, integrate\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "np.random.seed(0)\n",
    "#import deepwalk\n",
    "def readData(data_name,dir_name):\n",
    "    dir_path = dir_name + data_name+'/'\n",
    "    dataset_path = dir_path + data_name +'.mat'\n",
    "    \n",
    "    ##read data and process it\n",
    "    prepare_data = ReadData()\n",
    "    prepare_data.data_preparation(dataset_path, dir_path)\n",
    "    datasets_dict = prepare_data.read_dataset(dir_path,data_name)\n",
    "    x_train = datasets_dict[data_name][0]\n",
    "    y_train = datasets_dict[data_name][1]\n",
    "    x_test = datasets_dict[data_name][2]\n",
    "    y_test = datasets_dict[data_name][3]\n",
    "    x_train, x_test = prepare_data.z_norm(x_train, x_test)\n",
    "    nb_classes = prepare_data.num_classes(y_train,y_test)\n",
    "    y_train, y_test, y_true = prepare_data.on_hot_encode(y_train, y_test)\n",
    "    x_train, x_test, input_shape = prepare_data.reshape_x(x_train, x_test)  \n",
    "    #create train validation subvalidation sub set\n",
    "    x_training, x_validation = x_train[:90,:], x_train[90:,:]\n",
    "    y_training, y_validation = y_train[:90,:], y_train[90:,:]\n",
    "    \n",
    "    x_training = x_train\n",
    "    y_training = y_train\n",
    "    \n",
    "    return x_training, x_validation, x_test, y_training, y_validation, y_true, input_shape,nb_classes\n",
    "\n",
    "def trainModel(x_training, x_validation, y_training, y_validation,input_shape, nb_classes):\n",
    "    ##train the model\n",
    "    train_model = ConvNet()\n",
    "    #ResNet\n",
    "    #model = train_model.networkResNet(input_shape,nb_classes)\n",
    "    #FCN \n",
    "    model = train_model.network_fcN(input_shape,nb_classes)\n",
    "    #cnn\n",
    "    #model = train_model.network(input_shape,nb_classes)\n",
    "    print(model.summary())\n",
    "    train_model.trainNet(model,x_training,y_training,x_validation,y_validation,16,2000)\n",
    "    return model,train_model\n",
    "\n",
    "def predect(y_true,x_test,model,train_model,dimention_deactivated):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "    keras.backend.clear_session()\n",
    "    file = open('../Results/file_name.csv','a')\n",
    "    file.write(str(dimention_deactivated))\n",
    "    file.close()\n",
    "    df_metrics = train_model.calculate_metrics(y_true, y_pred, 0.0)\n",
    "    df = pandas.DataFrame(df_metrics).transpose()\n",
    "    df.to_csv('../Results/file_name.csv', mode='a')\n",
    "    return y_pred\n",
    "    \n",
    "def visulize_active_filter(model,x_test,y_true,nb_classes,train_model,cluster_centers,netLayers=3):\n",
    "    ##visulize activated filters for the original testing dataset\n",
    "    dimention_deactivated = 'all'\n",
    "    y_pred = predect(y_true,x_test,model,train_model,dimention_deactivated)\n",
    "    visulization = HighlyActivated(model,x_test,y_pred,nb_classes,netLayers=3)\n",
    "    activation_layers = visulization.Activated_filters(example_id=1)\n",
    "    visulization.get_high_activated_filters(activation_layers,dimention_deactivated)\n",
    "    activated_class_cluster = visulization.show_high_activated_period(activation_layers,dimention_deactivated,cluster_centers)\n",
    "    visulization.print_high_activated_combunation(activated_class_cluster)\n",
    "    ##visulize activated filters when set all dimention of the data to zero, and just one with its original data\n",
    "    x = []\n",
    "    combination_id = []\n",
    "    for i in range (x_test.shape[2]):\n",
    "        x.append(i)\n",
    "        tu = []\n",
    "        tu.append(i)\n",
    "        combination_id.append(tu)\n",
    "    \n",
    "    r = []\n",
    "    for i in range(2,x_test.shape[2]):\n",
    "        r.append(list(combinations(x, i)))\n",
    "        \n",
    "    for h in (r):\n",
    "        for l in h:\n",
    "            combination_id.append(l)\n",
    "    print(combination_id)\n",
    "    multivariate_variables = [[] for i in range(len(combination_id))]\n",
    "    for i in range(len(combination_id)):\n",
    "        multivariate_variables[i] = np.copy(x_test)\n",
    "        for j in range(len(multivariate_variables[i])):      \n",
    "            for k in range(len(multivariate_variables[i][j])):\n",
    "                for n in range(x_test.shape[2]):\n",
    "                    if (n not in combination_id[i]):\n",
    "                        multivariate_variables[i][j][k][n] = 0\n",
    "                    else:\n",
    "                        dimention_deactivated =  ''.join(map(str,combination_id[i])) \n",
    "        y_pred = predect(y_true,multivariate_variables[i],model,train_model,dimention_deactivated)\n",
    "        visulization = HighlyActivated(model,multivariate_variables[i],y_pred,nb_classes,netLayers=3)\n",
    "        activation_layers = visulization.Activated_filters(example_id=1)\n",
    "        visulization.get_high_activated_filters(activation_layers,dimention_deactivated)\n",
    "        activated_class_cluster = visulization.show_high_activated_period(activation_layers,dimention_deactivated,cluster_centers)\n",
    "        visulization.print_high_activated_combunation(activated_class_cluster)\n",
    "        \n",
    "def cluster_data_compenation(model,x_training,y_training,nb_classes):\n",
    "    visulization_traning = HighlyActivated(model,x_training,y_training,nb_classes,netLayers=3)\n",
    "    activation_layers = visulization_traning.Activated_filters(example_id=1)\n",
    "    period_indexes, filter_number = visulization_traning.get_high_active_period(activation_layers)\n",
    "    cluster_periods = visulization_traning.extract_dimention_active_period(period_indexes)\n",
    "    ##clustring the periods\n",
    "    cluster_data = []\n",
    "  \n",
    "    cluster_number = [12,11,13]\n",
    "    #clustering = Clustering(cluster_periods)\n",
    "    #print(new_data)\n",
    "    kshape = KShape(n_clusters=12, verbose=True, random_state=42)\n",
    "    trans_x = np.nan_to_num(cluster_periods[0])\n",
    "    kshape.fit(trans_x)\n",
    "    cluster_centers = kshape.cluster_centers_\n",
    "\n",
    "    cluster_data.append(cluster_centers)\n",
    "\n",
    "    x = []\n",
    "    combination_id = []\n",
    "    for i in range (x_training.shape[2]):\n",
    "        x.append(i)\n",
    "        tu = []\n",
    "        tu.append(i)\n",
    "        combination_id.append(tu)\n",
    "    \n",
    "    r = []\n",
    "    for i in range(2,x_training.shape[2]):\n",
    "        r.append(list(combinations(x, i)))\n",
    "        \n",
    "    for h in (r):\n",
    "        for l in h:\n",
    "            combination_id.append(l)\n",
    "    multivariate_variables = [[] for i in range(len(combination_id))]\n",
    "    for i in range(len(combination_id)):\n",
    "        multivariate_variables[i] = np.copy(x_training)\n",
    "        for j in range(len(multivariate_variables[i])):      \n",
    "            for k in range(len(multivariate_variables[i][j])):\n",
    "                for n in range(x_training.shape[2]):\n",
    "                    if (n not in combination_id[i]):\n",
    "                        multivariate_variables[i][j][k][n] = 0\n",
    "                    else:\n",
    "                        dimention_deactivated =  ''.join(map(str,combination_id[i])) \n",
    "        visulization_traning = HighlyActivated(model,x_training,y_training,nb_classes,netLayers=3)\n",
    "        activation_layers = visulization_traning.Activated_filters(example_id=1)\n",
    "        period_indexes, filter_number = visulization_traning.get_high_active_period(activation_layers)\n",
    "        cluster_periods = visulization_traning.extract_dimention_active_period(period_indexes)\n",
    "       \n",
    "        kshape = KShape(n_clusters=12, verbose=True, random_state=42)\n",
    "        trans_x = np.nan_to_num(cluster_periods[0])\n",
    "        kshape.fit(trans_x)\n",
    "        cluster_centers = kshape.cluster_centers_\n",
    "\n",
    "        cluster_data.append(cluster_centers)\n",
    "    \n",
    "   #save the cluster center for each layer in different array\n",
    "    cluser_center1 = []\n",
    "    cluser_center2 = []\n",
    "    cluser_center3 = []\n",
    "    cluser_center = []\n",
    "    l = 0\n",
    "    for i in cluster_data: \n",
    "        for j in i:\n",
    "            if(l == 0):\n",
    "                cluser_center1.append(j)\n",
    "            elif(l == 1):\n",
    "                cluser_center2.append(j)\n",
    "            else:\n",
    "                cluser_center3.append(j)\n",
    "        l +=1\n",
    "    cluser_center.append(cluser_center1)\n",
    "    cluser_center.append(cluser_center2)\n",
    "    cluser_center.append(cluser_center3)\n",
    "    #return cluser_center\n",
    "\n",
    "    return cluser_center\n",
    "\n",
    "\n",
    "def normilization(data):\n",
    "        i = 0\n",
    "        datt = []\n",
    "        maxi = max(data)\n",
    "        mini = abs(min(data))\n",
    "        while (i< len(data)):\n",
    "            \n",
    "            if(data[i] >=0):\n",
    "                val = data[i]/maxi\n",
    "            else:\n",
    "                val = data[i]/mini\n",
    "         \n",
    "            datt.append(val)\n",
    "            i += 1\n",
    "            \n",
    "        return datt\n",
    "\n",
    "#compare to cluster\n",
    "def fitted_cluster(data,cluster):\n",
    "        data = normilization(data)\n",
    "        cluster[0] = normilization(cluster[0])\n",
    "        mini = distance.euclidean(data,cluster[0])\n",
    "        cluster_id = 0\n",
    "        count = 0\n",
    "        for i in (cluster):\n",
    "            clu_nor = normilization(i)\n",
    "            dist = distance.euclidean(data,clu_nor)\n",
    "            if(dist < mini):\n",
    "                cluster_id = count\n",
    "                mini = dist\n",
    "            count+=1\n",
    "            \n",
    "        return cluster_id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 93, 13)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 93, 32)            3360      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 93, 32)            128       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 93, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 93, 64)            10304     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 93, 64)            256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 93, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 93, 128)           24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 93, 128)           512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 93, 128)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 40,554\n",
      "Trainable params: 40,106\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.6045 - accuracy: 0.8744\n",
      "Epoch 2/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.1412 - accuracy: 0.9742\n",
      "Epoch 3/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0859 - accuracy: 0.9823\n",
      "Epoch 4/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0592 - accuracy: 0.9867\n",
      "Epoch 5/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0431 - accuracy: 0.9903\n",
      "Epoch 6/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0359 - accuracy: 0.9929\n",
      "Epoch 7/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0260 - accuracy: 0.9950\n",
      "Epoch 8/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0273 - accuracy: 0.9933 0s - loss: 0.0276 - accuracy: \n",
      "Epoch 9/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0234 - accuracy: 0.9944\n",
      "Epoch 10/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0177 - accuracy: 0.9959\n",
      "Epoch 11/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0200 - accuracy: 0.9948\n",
      "Epoch 12/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0217 - accuracy: 0.9945\n",
      "Epoch 13/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0168 - accuracy: 0.9964\n",
      "Epoch 14/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0213 - accuracy: 0.9939\n",
      "Epoch 15/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0150 - accuracy: 0.9962\n",
      "Epoch 16/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0104 - accuracy: 0.9982\n",
      "Epoch 17/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0130 - accuracy: 0.9971\n",
      "Epoch 18/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0109 - accuracy: 0.9979\n",
      "Epoch 19/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0082 - accuracy: 0.9983\n",
      "Epoch 20/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0136 - accuracy: 0.9971\n",
      "Epoch 21/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0117 - accuracy: 0.9970\n",
      "Epoch 22/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0082 - accuracy: 0.9983\n",
      "Epoch 23/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0090 - accuracy: 0.9983\n",
      "Epoch 24/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0130 - accuracy: 0.9959\n",
      "Epoch 25/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0070 - accuracy: 0.9982\n",
      "Epoch 26/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0065 - accuracy: 0.9983\n",
      "Epoch 27/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0065 - accuracy: 0.9985\n",
      "Epoch 28/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0074 - accuracy: 0.9982\n",
      "Epoch 29/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0124 - accuracy: 0.9950\n",
      "Epoch 30/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0110 - accuracy: 0.9976\n",
      "Epoch 31/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0057 - accuracy: 0.9988\n",
      "Epoch 32/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0060 - accuracy: 0.9986\n",
      "Epoch 33/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0040 - accuracy: 0.9994\n",
      "Epoch 34/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0071 - accuracy: 0.9980\n",
      "Epoch 35/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0101 - accuracy: 0.9971\n",
      "Epoch 36/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0050 - accuracy: 0.9988\n",
      "Epoch 37/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0043 - accuracy: 0.9989\n",
      "Epoch 38/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0080 - accuracy: 0.9983\n",
      "Epoch 39/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0024 - accuracy: 0.9995\n",
      "Epoch 40/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0045 - accuracy: 0.9986\n",
      "Epoch 41/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0088 - accuracy: 0.9977\n",
      "Epoch 42/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0033 - accuracy: 0.9991\n",
      "Epoch 43/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0083 - accuracy: 0.9973\n",
      "Epoch 44/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0075 - accuracy: 0.9983\n",
      "Epoch 45/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0042 - accuracy: 0.9989\n",
      "Epoch 46/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0046 - accuracy: 0.9992\n",
      "Epoch 47/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0047 - accuracy: 0.9985\n",
      "Epoch 48/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 49/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0021 - accuracy: 0.9998\n",
      "Epoch 50/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0023 - accuracy: 0.9992\n",
      "Epoch 51/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0078 - accuracy: 0.9970\n",
      "Epoch 52/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0065 - accuracy: 0.9976\n",
      "Epoch 53/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0038 - accuracy: 0.9992\n",
      "Epoch 54/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0017 - accuracy: 0.9998\n",
      "Epoch 55/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 56/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0104 - accuracy: 0.9965\n",
      "Epoch 57/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0038 - accuracy: 0.9989\n",
      "Epoch 58/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 59/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0019 - accuracy: 0.9998\n",
      "Epoch 60/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0036 - accuracy: 0.9980\n",
      "Epoch 61/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0084 - accuracy: 0.9968\n",
      "Epoch 62/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0039 - accuracy: 0.9991\n",
      "Epoch 63/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 64/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0020 - accuracy: 0.9998\n",
      "Epoch 65/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 66/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0035 - accuracy: 0.9989\n",
      "Epoch 67/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 68/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0038 - accuracy: 0.9992\n",
      "Epoch 69/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0042 - accuracy: 0.9985\n",
      "Epoch 70/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0038 - accuracy: 0.9988\n",
      "Epoch 71/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0047 - accuracy: 0.9992\n",
      "Epoch 72/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0031 - accuracy: 0.9989\n",
      "Epoch 73/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0027 - accuracy: 0.9989\n",
      "Epoch 74/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 75/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 76/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0015 - accuracy: 0.9994\n",
      "Epoch 77/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 78/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0037 - accuracy: 0.9989\n",
      "Epoch 79/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0084 - accuracy: 0.9968\n",
      "Epoch 80/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 81/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 82/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0025 - accuracy: 0.9991\n",
      "Epoch 83/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0057 - accuracy: 0.9979\n",
      "Epoch 84/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0048 - accuracy: 0.9986\n",
      "Epoch 85/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 86/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0035 - accuracy: 0.9989\n",
      "Epoch 87/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 88/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 89/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0033 - accuracy: 0.9994\n",
      "Epoch 90/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0029 - accuracy: 0.9991\n",
      "Epoch 91/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0040 - accuracy: 0.9991\n",
      "Epoch 92/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0045 - accuracy: 0.9986\n",
      "Epoch 93/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0047 - accuracy: 0.9985\n",
      "Epoch 94/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 95/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0031 - accuracy: 0.9989\n",
      "Epoch 96/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 97/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0018 - accuracy: 0.9994\n",
      "Epoch 98/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0014 - accuracy: 0.9995\n",
      "Epoch 99/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0018 - accuracy: 0.9994\n",
      "Epoch 100/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0071 - accuracy: 0.9974\n",
      "Epoch 101/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 102/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0010 - accuracy: 0.9997\n",
      "Epoch 103/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0013 - accuracy: 0.9995\n",
      "Epoch 104/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 105/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0045 - accuracy: 0.9989\n",
      "Epoch 106/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0059 - accuracy: 0.9979 0s - loss: 0.0065 \n",
      "Epoch 107/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 4.8597e-04 - accuracy: 1.0000\n",
      "Epoch 108/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0016 - accuracy: 0.9992\n",
      "Epoch 109/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.9597e-04 - accuracy: 0.9998\n",
      "Epoch 110/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.6765e-04 - accuracy: 0.9998\n",
      "Epoch 111/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 112/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0025 - accuracy: 0.9989\n",
      "Epoch 113/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0027 - accuracy: 0.9994\n",
      "Epoch 114/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0047 - accuracy: 0.9983\n",
      "Epoch 115/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 116/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 117/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 118/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 119/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 9.3817e-04 - accuracy: 0.9998\n",
      "Epoch 120/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.9667e-04 - accuracy: 0.9998\n",
      "Epoch 121/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0063 - accuracy: 0.9974\n",
      "Epoch 122/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0047 - accuracy: 0.9985\n",
      "Epoch 123/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 124/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0019 - accuracy: 0.9991\n",
      "Epoch 125/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0039 - accuracy: 0.9988\n",
      "Epoch 126/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 127/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 4.5680e-04 - accuracy: 1.0000\n",
      "Epoch 128/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0052 - accuracy: 0.9982\n",
      "Epoch 129/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0028 - accuracy: 0.9991\n",
      "Epoch 130/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 131/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0026 - accuracy: 0.9995\n",
      "Epoch 132/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 133/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.4243e-04 - accuracy: 0.9998\n",
      "Epoch 134/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0038 - accuracy: 0.9991\n",
      "Epoch 135/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0021 - accuracy: 0.9994\n",
      "Epoch 136/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0037 - accuracy: 0.9983\n",
      "Epoch 137/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 138/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.5291e-04 - accuracy: 1.0000\n",
      "Epoch 139/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0024 - accuracy: 0.9991\n",
      "Epoch 140/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0024 - accuracy: 0.9992\n",
      "Epoch 141/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0026 - accuracy: 0.9986\n",
      "Epoch 142/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0020 - accuracy: 0.9995\n",
      "Epoch 143/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0024 - accuracy: 0.9997\n",
      "Epoch 144/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0050 - accuracy: 0.9982\n",
      "Epoch 145/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 7.7235e-04 - accuracy: 1.0000\n",
      "Epoch 146/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 147/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 148/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 7.0022e-04 - accuracy: 0.9998\n",
      "Epoch 149/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0066 - accuracy: 0.9983\n",
      "Epoch 150/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 8.5481e-04 - accuracy: 0.9997\n",
      "Epoch 151/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 8.7538e-04 - accuracy: 0.9997\n",
      "Epoch 152/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 4.4841e-04 - accuracy: 1.0000\n",
      "Epoch 153/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.7964e-04 - accuracy: 1.0000\n",
      "Epoch 154/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.5427e-04 - accuracy: 1.0000\n",
      "Epoch 155/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0029 - accuracy: 0.9994\n",
      "Epoch 156/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0026 - accuracy: 0.9992\n",
      "Epoch 157/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 158/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0018 - accuracy: 0.9995\n",
      "Epoch 159/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0023 - accuracy: 0.9991\n",
      "Epoch 160/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 161/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 3.6509e-04 - accuracy: 1.0000\n",
      "Epoch 162/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0013 - accuracy: 0.9995\n",
      "Epoch 163/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 6.1003e-04 - accuracy: 0.9998\n",
      "Epoch 164/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.4310e-04 - accuracy: 1.0000\n",
      "Epoch 165/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.3384e-04 - accuracy: 1.0000\n",
      "Epoch 166/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.0902e-04 - accuracy: 1.0000\n",
      "Epoch 167/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0034 - accuracy: 0.9989\n",
      "Epoch 168/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0035 - accuracy: 0.9988\n",
      "Epoch 169/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 170/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0018 - accuracy: 0.9989\n",
      "Epoch 171/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0027 - accuracy: 0.9989\n",
      "Epoch 172/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0022 - accuracy: 0.9989\n",
      "Epoch 173/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0011 - accuracy: 0.9994\n",
      "Epoch 174/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 6.0368e-04 - accuracy: 0.9998\n",
      "Epoch 175/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.2809e-04 - accuracy: 1.0000\n",
      "Epoch 176/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.9896e-04 - accuracy: 1.0000\n",
      "Epoch 177/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 178/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 6.9584e-04 - accuracy: 1.0000\n",
      "Epoch 179/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 5.8274e-04 - accuracy: 0.9997\n",
      "Epoch 180/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0027 - accuracy: 0.9991\n",
      "Epoch 181/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 182/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0011 - accuracy: 0.9994\n",
      "Epoch 183/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 8.6649e-04 - accuracy: 0.9997\n",
      "Epoch 184/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 8.0616e-04 - accuracy: 0.9997\n",
      "Epoch 185/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.0777e-04 - accuracy: 1.0000\n",
      "Epoch 186/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.3768e-04 - accuracy: 1.0000\n",
      "Epoch 187/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0017 - accuracy: 0.9994\n",
      "Epoch 188/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0051 - accuracy: 0.9982\n",
      "Epoch 189/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.5966e-04 - accuracy: 1.0000\n",
      "Epoch 190/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0025 - accuracy: 0.9992\n",
      "Epoch 191/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0010 - accuracy: 0.9995\n",
      "Epoch 192/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 6.2256e-04 - accuracy: 1.0000\n",
      "Epoch 193/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.9409e-04 - accuracy: 1.0000\n",
      "Epoch 194/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.9308e-04 - accuracy: 0.9998\n",
      "Epoch 195/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0028 - accuracy: 0.9991\n",
      "Epoch 196/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 9.7982e-04 - accuracy: 0.9997\n",
      "Epoch 197/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 198/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 199/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0012 - accuracy: 0.9995\n",
      "Epoch 200/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0030 - accuracy: 0.9994\n",
      "Epoch 201/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 202/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 203/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0016 - accuracy: 0.9994\n",
      "Epoch 204/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 5.9836e-04 - accuracy: 0.9998\n",
      "Epoch 205/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.6812e-04 - accuracy: 1.0000\n",
      "Epoch 206/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.6153e-04 - accuracy: 1.0000\n",
      "Epoch 207/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 208/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0062 - accuracy: 0.9974\n",
      "Epoch 209/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 5.0832e-04 - accuracy: 0.9998\n",
      "Epoch 210/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.5366e-04 - accuracy: 1.0000\n",
      "Epoch 211/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.1863e-04 - accuracy: 1.0000\n",
      "Epoch 212/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0025 - accuracy: 0.9994\n",
      "Epoch 213/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.7790e-04 - accuracy: 0.9997\n",
      "Epoch 214/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0012 - accuracy: 0.9994\n",
      "Epoch 215/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 9.9053e-04 - accuracy: 0.9995\n",
      "Epoch 216/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 5s 13ms/step - loss: 8.2849e-04 - accuracy: 0.9997\n",
      "Epoch 217/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0019 - accuracy: 0.9992\n",
      "Epoch 218/500\n",
      "413/413 [==============================] - 6s 15ms/step - loss: 0.0053 - accuracy: 0.9988\n",
      "Epoch 219/500\n",
      "413/413 [==============================] - 6s 14ms/step - loss: 0.0019 - accuracy: 0.9991\n",
      "Epoch 220/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 221/500\n",
      "413/413 [==============================] - 6s 14ms/step - loss: 0.0016 - accuracy: 0.9994\n",
      "Epoch 222/500\n",
      "413/413 [==============================] - 6s 13ms/step - loss: 6.7571e-04 - accuracy: 0.9997\n",
      "Epoch 223/500\n",
      "413/413 [==============================] - 6s 13ms/step - loss: 6.1998e-04 - accuracy: 0.9997\n",
      "Epoch 224/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0021 - accuracy: 0.9992\n",
      "Epoch 225/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0022 - accuracy: 0.9992\n",
      "Epoch 226/500\n",
      "413/413 [==============================] - 6s 13ms/step - loss: 5.9125e-04 - accuracy: 1.0000 0s - loss: 6.2007e\n",
      "Epoch 227/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0014 - accuracy: 0.9995\n",
      "Epoch 228/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 6.6440e-04 - accuracy: 0.9998\n",
      "Epoch 229/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0018 - accuracy: 0.9994\n",
      "Epoch 230/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0024 - accuracy: 0.9989\n",
      "Epoch 231/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0015 - accuracy: 0.9995A: 0s - loss:\n",
      "Epoch 232/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0010 - accuracy: 0.9998\n",
      "Epoch 233/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0011 - accuracy: 0.9995\n",
      "Epoch 234/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0018 - accuracy: 0.9994\n",
      "Epoch 235/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0011 - accuracy: 0.9995\n",
      "Epoch 236/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 8.9918e-04 - accuracy: 0.9997\n",
      "Epoch 237/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0021 - accuracy: 0.9995\n",
      "Epoch 238/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.0488e-04 - accuracy: 1.0000\n",
      "Epoch 239/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0016 - accuracy: 0.9997 0s -\n",
      "Epoch 240/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 8.6301e-04 - accuracy: 0.9998\n",
      "Epoch 241/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 5.5076e-04 - accuracy: 0.9997\n",
      "Epoch 242/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 9.9857e-04 - accuracy: 0.9997\n",
      "Epoch 243/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0027 - accuracy: 0.9991\n",
      "Epoch 244/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 6.0011e-04 - accuracy: 1.0000\n",
      "Epoch 245/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.5994e-04 - accuracy: 1.0000\n",
      "Epoch 246/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 1.2014e-04 - accuracy: 1.0000\n",
      "Epoch 247/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.7941e-04 - accuracy: 0.9997\n",
      "Epoch 248/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0028 - accuracy: 0.9991\n",
      "Epoch 249/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0024 - accuracy: 0.9991\n",
      "Epoch 250/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.7473e-04 - accuracy: 0.9998\n",
      "Epoch 251/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 1.9499e-04 - accuracy: 1.0000\n",
      "Epoch 252/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 5.2618e-04 - accuracy: 0.9998\n",
      "Epoch 253/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.8066e-04 - accuracy: 0.9998\n",
      "Epoch 254/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 1.7635e-04 - accuracy: 1.0000\n",
      "Epoch 255/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0013 - accuracy: 0.9994\n",
      "Epoch 256/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.2210e-04 - accuracy: 1.0000\n",
      "Epoch 257/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 6.9631e-05 - accuracy: 1.0000\n",
      "Epoch 258/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.3936e-04 - accuracy: 0.9998\n",
      "Epoch 259/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.1808e-04 - accuracy: 1.0000\n",
      "Epoch 260/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0046 - accuracy: 0.9986\n",
      "Epoch 261/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0028 - accuracy: 0.9992\n",
      "Epoch 262/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 263/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 8.8594e-04 - accuracy: 0.9997\n",
      "Epoch 264/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.5194e-04 - accuracy: 1.0000\n",
      "Epoch 265/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 3.6330e-04 - accuracy: 0.9998\n",
      "Epoch 266/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.7952e-04 - accuracy: 1.0000\n",
      "Epoch 267/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.6701e-04 - accuracy: 0.9998\n",
      "Epoch 268/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 8.3366e-05 - accuracy: 1.0000\n",
      "Epoch 269/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0057 - accuracy: 0.9985\n",
      "Epoch 270/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0019 - accuracy: 0.9992\n",
      "Epoch 271/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 272/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 1.5993e-04 - accuracy: 1.0000 1s - loss: 1.5297e-04 - accuracy: 1.00 -\n",
      "Epoch 273/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.1609e-04 - accuracy: 1.0000\n",
      "Epoch 274/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.4842e-04 - accuracy: 1.0000\n",
      "Epoch 275/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 1.2151e-04 - accuracy: 1.0000\n",
      "Epoch 276/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 5.0520e-05 - accuracy: 1.0000\n",
      "Epoch 277/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 1.3632e-04 - accuracy: 1.0000\n",
      "Epoch 278/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 4.0783e-04 - accuracy: 1.0000\n",
      "Epoch 279/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0019 - accuracy: 0.9995\n",
      "Epoch 280/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0030 - accuracy: 0.9995\n",
      "Epoch 281/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0012 - accuracy: 0.9992\n",
      "Epoch 282/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0022 - accuracy: 0.9995\n",
      "Epoch 283/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 3.2758e-04 - accuracy: 0.9998\n",
      "Epoch 284/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 7.1428e-05 - accuracy: 1.0000\n",
      "Epoch 285/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.4518e-04 - accuracy: 1.0000\n",
      "Epoch 286/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0013 - accuracy: 0.9994\n",
      "Epoch 287/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 9.8432e-04 - accuracy: 0.9998\n",
      "Epoch 288/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0010 - accuracy: 0.9995\n",
      "Epoch 289/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 290/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0012 - accuracy: 0.9995\n",
      "Epoch 291/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 292/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 293/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 5.2902e-04 - accuracy: 0.9998 0s - loss: 5.5018e-04 - ac\n",
      "Epoch 294/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.9630e-04 - accuracy: 0.9998\n",
      "Epoch 295/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 1.5701e-04 - accuracy: 1.0000\n",
      "Epoch 296/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 8.2152e-05 - accuracy: 1.0000\n",
      "Epoch 297/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0021 - accuracy: 0.9991\n",
      "Epoch 298/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.9568e-04 - accuracy: 0.9998\n",
      "Epoch 299/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 4.5235e-04 - accuracy: 0.9998\n",
      "Epoch 300/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0014 - accuracy: 0.9995\n",
      "Epoch 301/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 302/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 6.5427e-04 - accuracy: 0.9998\n",
      "Epoch 303/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0010 - accuracy: 0.9994\n",
      "Epoch 304/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 305/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0013 - accuracy: 0.9995\n",
      "Epoch 306/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 1.5526e-04 - accuracy: 1.0000\n",
      "Epoch 307/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.5635e-04 - accuracy: 0.9998\n",
      "Epoch 308/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0036 - accuracy: 0.9991\n",
      "Epoch 309/500\n",
      "413/413 [==============================] - 6s 14ms/step - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 310/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 3.0022e-04 - accuracy: 1.0000\n",
      "Epoch 311/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 9.1145e-04 - accuracy: 0.9995\n",
      "Epoch 312/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 313/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0020 - accuracy: 0.9998\n",
      "Epoch 314/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0019 - accuracy: 0.9992\n",
      "Epoch 315/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0015 - accuracy: 0.9994\n",
      "Epoch 316/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.2150e-04 - accuracy: 0.9997\n",
      "Epoch 317/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0013 - accuracy: 0.9994\n",
      "Epoch 318/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 7.0604e-04 - accuracy: 0.9997\n",
      "Epoch 319/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 320/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.3037e-04 - accuracy: 1.0000\n",
      "Epoch 321/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 1.1975e-04 - accuracy: 1.0000\n",
      "Epoch 322/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.1421e-04 - accuracy: 0.9997\n",
      "Epoch 323/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0017 - accuracy: 0.9992\n",
      "Epoch 324/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0011 - accuracy: 0.9995\n",
      "Epoch 325/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0016 - accuracy: 0.9994\n",
      "Epoch 326/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 327/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.1480e-04 - accuracy: 0.9998\n",
      "Epoch 328/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.7524e-04 - accuracy: 1.0000\n",
      "Epoch 329/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 3.7273e-04 - accuracy: 0.9998\n",
      "Epoch 330/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 5.4436e-04 - accuracy: 0.9995\n",
      "Epoch 331/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 332/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.4997e-04 - accuracy: 0.9998\n",
      "Epoch 333/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0032 - accuracy: 0.9991\n",
      "Epoch 334/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0039 - accuracy: 0.9986\n",
      "Epoch 335/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0020 - accuracy: 0.9991\n",
      "Epoch 336/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 337/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 8.2731e-04 - accuracy: 0.9998\n",
      "Epoch 338/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.3855e-04 - accuracy: 1.0000\n",
      "Epoch 339/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.0604e-04 - accuracy: 1.0000\n",
      "Epoch 340/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 3.6703e-05 - accuracy: 1.0000\n",
      "Epoch 341/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.6385e-04 - accuracy: 0.9998\n",
      "Epoch 342/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 343/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 4.3817e-04 - accuracy: 0.9998\n",
      "Epoch 344/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 4.8448e-04 - accuracy: 0.9998\n",
      "Epoch 345/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.3689e-04 - accuracy: 0.9998\n",
      "Epoch 346/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.3830e-04 - accuracy: 1.0000\n",
      "Epoch 347/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 1.8542e-04 - accuracy: 1.0000\n",
      "Epoch 348/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 4.9791e-05 - accuracy: 1.0000\n",
      "Epoch 349/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.8525e-04 - accuracy: 0.9998\n",
      "Epoch 350/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0024 - accuracy: 0.9994\n",
      "Epoch 351/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0011 - accuracy: 0.9995\n",
      "Epoch 352/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0012 - accuracy: 0.9995\n",
      "Epoch 353/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.5367e-04 - accuracy: 1.0000\n",
      "Epoch 354/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0030 - accuracy: 0.9989\n",
      "Epoch 355/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0024 - accuracy: 0.9989\n",
      "Epoch 356/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.2025e-04 - accuracy: 1.0000\n",
      "Epoch 357/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 6.8008e-04 - accuracy: 0.9997\n",
      "Epoch 358/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.9925e-04 - accuracy: 1.0000\n",
      "Epoch 359/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 4.7138e-05 - accuracy: 1.0000\n",
      "Epoch 360/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.4809e-04 - accuracy: 1.0000\n",
      "Epoch 361/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.7120e-04 - accuracy: 1.0000\n",
      "Epoch 362/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 4.5252e-05 - accuracy: 1.0000\n",
      "Epoch 363/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.3655e-05 - accuracy: 1.0000 2s - ETA: 1s -\n",
      "Epoch 364/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.9011e-04 - accuracy: 1.0000\n",
      "Epoch 365/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 3.1323e-04 - accuracy: 0.9998\n",
      "Epoch 366/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.7800e-04 - accuracy: 1.0000\n",
      "Epoch 367/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0036 - accuracy: 0.9982\n",
      "Epoch 368/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 5s 12ms/step - loss: 7.1837e-04 - accuracy: 0.9998\n",
      "Epoch 369/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.3038e-04 - accuracy: 0.9998\n",
      "Epoch 370/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.8024e-04 - accuracy: 0.9997\n",
      "Epoch 371/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 6.6484e-05 - accuracy: 1.0000\n",
      "Epoch 372/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0010 - accuracy: 0.9995\n",
      "Epoch 373/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 374/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 2.3213e-04 - accuracy: 1.0000\n",
      "Epoch 375/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.7944e-05 - accuracy: 1.0000\n",
      "Epoch 376/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.9903e-04 - accuracy: 1.0000\n",
      "Epoch 377/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.2828e-05 - accuracy: 1.0000\n",
      "Epoch 378/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0020 - accuracy: 0.9994\n",
      "Epoch 379/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0037 - accuracy: 0.9988\n",
      "Epoch 380/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0011 - accuracy: 0.9994\n",
      "Epoch 381/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0012 - accuracy: 0.9998\n",
      "Epoch 382/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 4.0172e-04 - accuracy: 0.9998\n",
      "Epoch 383/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.5297e-04 - accuracy: 1.0000\n",
      "Epoch 384/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0013 - accuracy: 0.9994\n",
      "Epoch 385/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0013 - accuracy: 0.9995\n",
      "Epoch 386/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 4.5184e-04 - accuracy: 0.9998\n",
      "Epoch 387/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 1.7609e-04 - accuracy: 1.0000\n",
      "Epoch 388/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0011 - accuracy: 0.9994\n",
      "Epoch 389/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 6.5967e-04 - accuracy: 0.9997\n",
      "Epoch 390/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.0427e-05 - accuracy: 1.0000\n",
      "Epoch 391/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0030 - accuracy: 0.9991\n",
      "Epoch 392/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 1.5746e-04 - accuracy: 1.0000\n",
      "Epoch 393/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.6529e-04 - accuracy: 1.0000\n",
      "Epoch 394/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.5693e-04 - accuracy: 1.0000\n",
      "Epoch 395/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 9.9312e-05 - accuracy: 1.0000\n",
      "Epoch 396/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.7370e-05 - accuracy: 1.0000\n",
      "Epoch 397/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.2832e-04 - accuracy: 1.0000\n",
      "Epoch 398/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 9.1480e-05 - accuracy: 1.0000\n",
      "Epoch 399/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.1632e-05 - accuracy: 1.0000\n",
      "Epoch 400/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0017 - accuracy: 0.9994\n",
      "Epoch 401/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 402/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.3599e-04 - accuracy: 0.9998\n",
      "Epoch 403/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.7152e-04 - accuracy: 0.9998\n",
      "Epoch 404/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 3.6713e-04 - accuracy: 0.9998\n",
      "Epoch 405/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0014 - accuracy: 0.9994\n",
      "Epoch 406/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0026 - accuracy: 0.9994\n",
      "Epoch 407/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0018 - accuracy: 0.9994\n",
      "Epoch 408/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0027 - accuracy: 0.9991\n",
      "Epoch 409/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 3.2429e-04 - accuracy: 1.0000\n",
      "Epoch 410/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 3.5200e-04 - accuracy: 1.0000\n",
      "Epoch 411/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.2870e-04 - accuracy: 1.0000\n",
      "Epoch 412/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 7.7539e-04 - accuracy: 0.9997\n",
      "Epoch 413/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.0091e-04 - accuracy: 1.0000\n",
      "Epoch 414/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0022 - accuracy: 0.9994\n",
      "Epoch 415/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 416/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 3.4337e-04 - accuracy: 1.0000\n",
      "Epoch 417/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 8.0953e-04 - accuracy: 0.9998\n",
      "Epoch 418/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 7.2200e-04 - accuracy: 0.9998\n",
      "Epoch 419/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0015 - accuracy: 0.9992\n",
      "Epoch 420/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.1952e-05 - accuracy: 1.0000\n",
      "Epoch 421/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.4736e-04 - accuracy: 1.0000\n",
      "Epoch 422/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0014 - accuracy: 0.9994\n",
      "Epoch 423/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0022 - accuracy: 0.9992\n",
      "Epoch 424/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0016 - accuracy: 0.9992\n",
      "Epoch 425/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.1188e-04 - accuracy: 0.9998\n",
      "Epoch 426/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 427/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.4108e-04 - accuracy: 0.9997\n",
      "Epoch 428/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.2965e-04 - accuracy: 1.0000\n",
      "Epoch 429/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 1.8823e-04 - accuracy: 1.0000\n",
      "Epoch 430/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0036 - accuracy: 0.9988\n",
      "Epoch 431/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 8.7400e-04 - accuracy: 0.9997\n",
      "Epoch 432/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 4.2187e-04 - accuracy: 0.9997\n",
      "Epoch 433/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.2505e-04 - accuracy: 1.0000\n",
      "Epoch 434/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 8.1726e-05 - accuracy: 1.0000\n",
      "Epoch 435/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 8.3962e-05 - accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.0770e-04 - accuracy: 1.0000\n",
      "Epoch 437/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 4.8837e-04 - accuracy: 0.9998\n",
      "Epoch 438/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.1535e-05 - accuracy: 1.0000\n",
      "Epoch 439/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 4.6731e-04 - accuracy: 0.9998\n",
      "Epoch 440/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.3582e-04 - accuracy: 0.9997\n",
      "Epoch 441/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0015 - accuracy: 0.9995\n",
      "Epoch 442/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 1.7018e-04 - accuracy: 1.0000\n",
      "Epoch 443/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.0815e-04 - accuracy: 0.9998\n",
      "Epoch 444/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0013 - accuracy: 0.9994\n",
      "Epoch 445/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 1.1999e-04 - accuracy: 1.0000\n",
      "Epoch 446/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 6.5508e-04 - accuracy: 0.9998\n",
      "Epoch 447/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0024 - accuracy: 0.9992\n",
      "Epoch 448/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 8.9814e-05 - accuracy: 1.0000\n",
      "Epoch 449/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0023 - accuracy: 0.9995\n",
      "Epoch 450/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 6.1473e-04 - accuracy: 0.9997\n",
      "Epoch 451/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 8.4597e-04 - accuracy: 0.9995\n",
      "Epoch 452/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.4545e-04 - accuracy: 0.9998\n",
      "Epoch 453/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.2101e-04 - accuracy: 1.0000 0s - loss: 1.2642e-04 - accura\n",
      "Epoch 454/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 1.7882e-04 - accuracy: 1.0000\n",
      "Epoch 455/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 2.4302e-04 - accuracy: 1.0000\n",
      "Epoch 456/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0017 - accuracy: 0.9992\n",
      "Epoch 457/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0011 - accuracy: 0.9995\n",
      "Epoch 458/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.1105e-04 - accuracy: 1.0000\n",
      "Epoch 459/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 6.4582e-04 - accuracy: 0.9998\n",
      "Epoch 460/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0031 - accuracy: 0.9994\n",
      "Epoch 461/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 4.9552e-04 - accuracy: 0.9998\n",
      "Epoch 462/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0013 - accuracy: 0.9998\n",
      "Epoch 463/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.8171e-04 - accuracy: 0.9997\n",
      "Epoch 464/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 1.3032e-04 - accuracy: 1.0000\n",
      "Epoch 465/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.7885e-04 - accuracy: 0.9998\n",
      "Epoch 466/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 4.5642e-05 - accuracy: 1.0000\n",
      "Epoch 467/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.7394e-04 - accuracy: 1.0000\n",
      "Epoch 468/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.0041e-04 - accuracy: 0.9998\n",
      "Epoch 469/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0013 - accuracy: 0.9995\n",
      "Epoch 470/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0011 - accuracy: 0.9995\n",
      "Epoch 471/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.4625e-04 - accuracy: 0.9998\n",
      "Epoch 472/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0015 - accuracy: 0.9994\n",
      "Epoch 473/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0023 - accuracy: 0.9992\n",
      "Epoch 474/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.4579e-04 - accuracy: 1.0000\n",
      "Epoch 475/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 5.3456e-04 - accuracy: 0.9998\n",
      "Epoch 476/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 3.7842e-04 - accuracy: 0.9998\n",
      "Epoch 477/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0016 - accuracy: 0.9995\n",
      "Epoch 478/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 5.6611e-04 - accuracy: 0.9998\n",
      "Epoch 479/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 9.0274e-05 - accuracy: 1.0000\n",
      "Epoch 480/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 2.0020e-04 - accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 8.9720e-05 - accuracy: 1.0000\n",
      "Epoch 482/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 7.3906e-05 - accuracy: 1.0000\n",
      "Epoch 483/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0013 - accuracy: 0.9997\n",
      "Epoch 484/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 5.2116e-04 - accuracy: 0.9998\n",
      "Epoch 485/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.1056e-04 - accuracy: 0.9997\n",
      "Epoch 486/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0028 - accuracy: 0.9989\n",
      "Epoch 487/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0014 - accuracy: 0.9994\n",
      "Epoch 488/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 2.1891e-04 - accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 1.1649e-04 - accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 7.7217e-05 - accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 0.0013 - accuracy: 0.9995\n",
      "Epoch 492/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 0.0018 - accuracy: 0.9995\n",
      "Epoch 493/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 3.9266e-04 - accuracy: 0.9998\n",
      "Epoch 494/500\n",
      "413/413 [==============================] - 5s 11ms/step - loss: 5.0885e-04 - accuracy: 0.9998\n",
      "Epoch 495/500\n",
      "413/413 [==============================] - 5s 12ms/step - loss: 6.0255e-04 - accuracy: 0.9998\n",
      "Epoch 496/500\n",
      "413/413 [==============================] - 6s 14ms/step - loss: 6.8732e-04 - accuracy: 0.9997\n",
      "Epoch 497/500\n",
      "413/413 [==============================] - 6s 15ms/step - loss: 5.8051e-04 - accuracy: 0.9998\n",
      "Epoch 498/500\n",
      "413/413 [==============================] - 6s 13ms/step - loss: 0.0017 - accuracy: 0.9995\n",
      "Epoch 499/500\n",
      "413/413 [==============================] - 6s 14ms/step - loss: 2.0497e-04 - accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "413/413 [==============================] - 5s 13ms/step - loss: 2.4796e-04 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "data_name = 'ArabicDigits'\n",
    "dir_name = '../../Paper1/Data/mtsdata/'\n",
    "x_training, x_validation, x_test, y_training, y_validation, y_true,input_shape, nb_classes = readData(data_name,dir_name)\n",
    "model,train_model = trainModel(x_training, x_validation, y_training, y_validation,input_shape, nb_classes)\n",
    "y_pred = predect(y_true,x_test,model,train_model,'alls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6510, 93, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Nov 12 14:24:41 2020\n",
    "\n",
    "@author: raneen_pc\n",
    "\n",
    "This class will return the highly activated filters(feature map) for each layer in anetwork\n",
    "\"\"\"\n",
    "import tensorflow.keras as keras\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from scipy import stats, integrate\n",
    "from scipy.interpolate import interp1d\n",
    "import seaborn as sns\n",
    "from scipy.spatial import distance\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import ttest_1samp\n",
    "import scipy.stats as stats\n",
    "import bisect\n",
    "np.random.seed(0)\n",
    "\n",
    "class HighlyActivated:\n",
    "    def __init__(self, model,train_model,test_data,y_pred,nb_classes,netLayers):\n",
    "        self.model = model\n",
    "        self.x_test = test_data\n",
    "        self.y_pred = y_pred\n",
    "        self.nb_classes = nb_classes\n",
    "        self.netLayers = netLayers\n",
    "        self.train_model = train_model\n",
    "        \n",
    "    def Activated_filters(self,example_id):\n",
    "        #layer_outputs = [layer.output for layer in self.model.layers[:self.netLayers+3]] \n",
    "        layer_outputs = [layer.output for layer in self.model.layers[:self.netLayers+6]] \n",
    "        # Extracts the outputs of the top n layers\n",
    "        # Creates a model that will return these outputs, given the model input\n",
    "        activation_model = keras.models.Model(inputs=self.model.input, outputs=layer_outputs) \n",
    "        activations = activation_model.predict(self.x_test)\n",
    "        #shows the activated filters for each layer for an example\n",
    "\n",
    "        #for i in range(0,self.netLayers+3):\n",
    "        for i in range(0,self.netLayers+3):\n",
    "            flag = False\n",
    "            if i == 0:\n",
    "            #or i == 1:\n",
    "                activated_nodes = activations[i]\n",
    "                flag = True\n",
    "            #elif(i%2 == 1 and i >1):\n",
    "            #    activated_nodes = activations[i]\n",
    "            #    flag = True\n",
    "            if(flag):\n",
    "                n_filters, ix = activated_nodes.shape[2], 1\n",
    "                for j in range(0,n_filters):\n",
    "                        # specify subplot and turn of axis\n",
    "                        ax = pyplot.subplot(n_filters, 3, ix)\n",
    "                        ax.set_xticks([])\n",
    "                        ax.set_yticks([])\n",
    "                        # plot filter channel\n",
    "                        pyplot.plot(activated_nodes[example_id, :, j])\n",
    "                        ix += 1\n",
    "                pyplot.show()     \n",
    "        return activations\n",
    "                \n",
    "    def get_best_distribution(sel,data):\n",
    "        dist_names = [\"norm\", \"exponweib\", \"weibull_max\", \"weibull_min\", \"pareto\", \"genextreme\"]\n",
    "        dist_results = []\n",
    "        params = {}\n",
    "        for dist_name in dist_names:\n",
    "            dist = getattr(stats, dist_name)\n",
    "            param = dist.fit(data)\n",
    "        params[dist_name] = param\n",
    "        # Applying the Kolmogorov-Smirnov test\n",
    "        D, p = stats.kstest(data, dist_name, args=param)\n",
    "        dist_results.append((dist_name, p))\n",
    "        # select the best fitted distribution\n",
    "        best_dist, best_p = (max(dist_results, key=lambda item: item[1]))\n",
    "        return best_dist, best_p, params[best_dist]\n",
    "    \n",
    "    def change_dimention(self,x_test,num_dim):\n",
    "        new_data = np.copy(x_test)\n",
    "        for xe in range(len(x_test)):\n",
    "            for i in range(len(new_data[xe])):\n",
    "                    for n in num_dim:  \n",
    "                            new_data[xe][i][n]=0  \n",
    "        return new_data\n",
    "    \n",
    "    def predect(self,y_true,x_test,model):\n",
    "        y_pred = model.predict(x_test)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        keras.backend.clear_session()        \n",
    "        return y_pred\n",
    "    \n",
    "    def get_dimention_MHAP(self,x_test):\n",
    "        #dimention = [[] for i in range(self.nb_classes)]\n",
    "        x_test_label = [[] for i in range(self.nb_classes)]\n",
    "        y_test_label = [[] for i in range(self.nb_classes)]\n",
    "        x = []\n",
    "        x_test_copy = np.copy(x_test)\n",
    "        combination_id = []\n",
    "        for i in range (x_test_copy.shape[2]):\n",
    "                x.append(i)\n",
    "                tu = []\n",
    "                tu.append(i)\n",
    "                combination_id.append(tu)\n",
    "            \n",
    "        r = []\n",
    "        for i in range(2,x_test_copy.shape[2]):\n",
    "            r.append(list(combinations(x, i)))\n",
    "                \n",
    "        for h in (r):\n",
    "            for l in h:\n",
    "                combination_id.append(l)\n",
    "        \n",
    "        #combination_id all posible combination of the signal data\n",
    "        #dim_test = []\n",
    "        for z in range(len(self.y_pred)):\n",
    "            index_label = self.y_pred[z]\n",
    "            i = index_label.tolist().index(1) # i will return index of 2\n",
    "            index_label = i\n",
    "            x_test_label[index_label].append(x_test_copy[z])\n",
    "            y_test_label[index_label].append(self.y_pred[z])\n",
    "        \n",
    "        combination_ids = []\n",
    "        for i in combination_id:\n",
    "            combination_ids.append(list(i))\n",
    "        result_acc = []\n",
    "        for i in combination_ids:\n",
    "            ind = 0\n",
    "            for js in x_test_label:\n",
    "                xx = self.change_dimention(js,i)\n",
    "                rounded_labels=np.argmax(y_test_label[ind], axis=1)\n",
    "                y_pred = self.predect(rounded_labels,xx,self.model)\n",
    "                result_acc.append(accuracy_score(rounded_labels, y_pred))\n",
    "                ind +=1\n",
    "        return result_acc\n",
    "    \n",
    "    def define_threshould(self,activations):\n",
    "        threshoulds = [[] for i in range(self.netLayers)]\n",
    "        layer_data = [[] for i in range(self.netLayers)]\n",
    "        ff = True\n",
    "        for j in range(len(self.x_test)):\n",
    "            activated_id = 1\n",
    "            filter_index = 0\n",
    "            #layer_fil = []\n",
    "            for l in range(1,self.netLayers+6):\n",
    "                flag = False\n",
    "                #get the id of the filter layer\n",
    "                if(l == 1):\n",
    "                    activated_id = 0\n",
    "                    filter_index = 0\n",
    "                    flag = True                    \n",
    "                elif(l==4):\n",
    "                    activated_id = 1\n",
    "                    filter_index = 1\n",
    "                    flag = True  \n",
    "                elif(l==7):\n",
    "                    activated_id = 2\n",
    "                    filter_index = 2\n",
    "                    flag = True\n",
    "                #so make sure that we only have the id of conv layer\n",
    "                if(flag):\n",
    "                    activated_nodes = activations[l]\n",
    "                    channel = [[] for i in range(activated_nodes.shape[2])]\n",
    "                    #print(len(activated_nodes))\n",
    "                    \n",
    "                    for i in range(0,activated_nodes.shape[2]):\n",
    "                        for k in (activated_nodes[j, :, i]):\n",
    "                            channel[i].append(k)\n",
    "                    \n",
    "                    layer_data[filter_index].append(channel)\n",
    "       \n",
    "        #each layer has n-data and each n data has m filters \n",
    "        layer_data1 = [[] for i in range(self.netLayers)]\n",
    "        channel_len = [32,64,128]\n",
    "        for i in range(len(layer_data)):\n",
    "            for l in range((channel_len[i])):\n",
    "                layer_data1[i].append([])\n",
    "            for j in range(len(layer_data[i])):\n",
    "                for k in range(len(layer_data[i][j])):\n",
    "                    for n in layer_data[i][j][k]:\n",
    "                        layer_data1[i][k].append(n)\n",
    "        #now we want to define a threshold for each filter of each array \n",
    "        for i in range(len(threshoulds)):\n",
    "            mean_val = 0\n",
    "            std_val = 0\n",
    "            for l in range((channel_len[i])):\n",
    "                threshoulds[i].append([])\n",
    "                mean_val = np.mean(layer_data1[i][l])\n",
    "                std_val = np.std(layer_data1[i][l])\n",
    "                Q3 = np.percentile(layer_data1[i][l], 98)\n",
    "                normal=stats.norm(loc=mean_val, scale=std_val)\n",
    "                threshoulds[i][l].append(mean_val)\n",
    "                threshoulds[i][l].append(std_val)\n",
    "                threshoulds[i][l].append(normal)\n",
    "                threshoulds[i][l].append(Q3)\n",
    "                \n",
    "        return threshoulds\n",
    "    def define_threshould_all_filters(self,activations):\n",
    "        threshoulds = [[] for i in range(self.netLayers)]\n",
    "        layer_data = [[] for i in range(self.netLayers)]\n",
    "        ff = True\n",
    "        for j in range(len(self.x_test)):\n",
    "            activated_id = 1\n",
    "            filter_index = 0\n",
    "            #layer_fil = []\n",
    "            for l in range(1,self.netLayers+6):\n",
    "                flag = False\n",
    "                #get the id of the filter layer\n",
    "                if(l == 1):\n",
    "                    activated_id = 0\n",
    "                    filter_index = 0\n",
    "                    flag = True                    \n",
    "                elif(l==4):\n",
    "                    activated_id = 1\n",
    "                    filter_index = 1\n",
    "                    flag = True  \n",
    "                elif(l==7):\n",
    "                    activated_id = 2\n",
    "                    filter_index = 2\n",
    "                    flag = True\n",
    "                #so make sure that we only have the id of conv layer\n",
    "                if(flag):\n",
    "                    activated_nodes = activations[l]\n",
    "                    #channel = [[] for i in range(activated_nodes.shape[2])]\n",
    "                    print(activated_nodes[j, :, i])\n",
    "                    for i in range(0,activated_nodes.shape[2]):\n",
    "                        for k in (activated_nodes[j, :, i]):\n",
    "                            layer_data[filter_index].append(k)\n",
    "                \n",
    "        for i in range(len(threshoulds)):\n",
    "            data_int = layer_data[i]\n",
    "            sorted_integers = sorted(data_int)\n",
    "            #for l in (len(layer_data[i])):\n",
    "            mean_val = np.mean(layer_data[i])\n",
    "            std_val = np.std(layer_data[i])\n",
    "            Q3 = np.percentile(layer_data[i], 98)\n",
    "            normal=stats.norm(loc=mean_val, scale=std_val)\n",
    "            threshoulds[i].append(mean_val)\n",
    "            threshoulds[i].append(std_val)\n",
    "            threshoulds[i].append(normal)\n",
    "            threshoulds[i].append(Q3)\n",
    "            threshoulds[i].append(sorted_integers[-10])      \n",
    "                \n",
    "        return threshoulds\n",
    "    \n",
    "    def get_index_MHAP(self,activations,kernal_size=[]):\n",
    "        #depends on the network archi\n",
    "        #kernal_size = [8,5,3]\n",
    "        kernal_size= kernal_size\n",
    "        #initilize the array of the output (size will be based on output class label)\n",
    "        #classes_lists = [[] for i in range(self.nb_classes)]\n",
    "        filter_lists = [[] for i in range(self.netLayers)]\n",
    "        filter_lists_index = [[] for i in range(self.netLayers)]\n",
    "        period_active = []\n",
    "        index_period_active = []\n",
    "        #threshoulds = self.define_threshould_all_filters(activation_layers)\n",
    "        threshoulds = self.define_threshould(activation_layers)\n",
    "        #classes_lists_period = [[] for i in range(self.nb_classes)]\n",
    "        #loop through each training sample\n",
    "        for j in range(len(self.x_test)):\n",
    "            #loop through each layer in the network\n",
    "            # we need to take only the conv layers(not the conv and pooling)\n",
    "            #so we take each second index of the loop\n",
    "            activated_id = 1\n",
    "            #len of the period_will depend on the lyer kernal size (first will be same and then multily of previous)\n",
    "            Len_period = 8\n",
    "            filter_index = 0\n",
    "            #layer_fil = []\n",
    "            for l in range(1,self.netLayers+6):\n",
    "                flag = False\n",
    "                #get the id of the filter layer\n",
    "                if(l == 1):\n",
    "                    Len_period = kernal_size[0]\n",
    "                    filter_index = 0\n",
    "                    flag = True                    \n",
    "                elif(l==4):\n",
    "                    Len_period = kernal_size[0] * kernal_size[1]\n",
    "                    filter_index = 1\n",
    "                    flag = True  \n",
    "                elif(l==7):\n",
    "                    Len_period = kernal_size[0]*kernal_size[1]*kernal_size[2]\n",
    "                    filter_index = 2\n",
    "                    flag = True\n",
    "                #so make sure that we only have the id of conv layer\n",
    "                if(flag):\n",
    "                    #active id is what conv layer of network, \n",
    "                    #print(activated_id)\n",
    "                    activated_nodes = activations[l]\n",
    "                    #each filter of the data(activated_nodes.shape[2]) has (n values based on strid(inour case =1))                       \n",
    "                    for i in range(0,activated_nodes.shape[2]):\n",
    "                        index_k = 0\n",
    "                        #loop throug the feature map\n",
    "                        val = 0.8\n",
    "                        for k in (activated_nodes[j, :, i]):\n",
    "                            if(filter_index == 0):\n",
    "                                val = 0.07\n",
    "                            elif(filter_index==1):\n",
    "                                #print(threshoulds[filter_index][i][2].pdf(k))\n",
    "                                val = 0.15\n",
    "                            else:\n",
    "                                val = 0.05\n",
    "                            #if(k >= threshoulds[filter_index][4]):\n",
    "                            if(threshoulds[filter_index][i][2].pdf(k) >val):\n",
    "                                #here pertubipt data from this index to len of period and get feature as mhap\n",
    "                                #dimentions = self.get_dimention_MHAP(j,index_k,Len_period)\n",
    "                                #dimentions = [1]\n",
    "                                #here have an array to the periods, will be same as order(original traning data)\n",
    "                                #each sample data will have periods from each dimention, for each layer\n",
    "                                #[[[l1_p],[l2_p],[l3_p]],.....]\n",
    "                                #print('ccc')\n",
    "                                d1 =[]\n",
    "                                #d2 =[]\n",
    "                                #d3 =[]\n",
    "                                #get each dimention data\n",
    "                                for xe in(self.x_test[j]):\n",
    "                                    d1.append(xe[0])\n",
    "                                    #d2.append(xe[1])\n",
    "                                    #d3.append(xe[2])\n",
    "                                mhap1 = []\n",
    "                                #mhap2 = []\n",
    "                                #mhap3 = []\n",
    "                                if(index_k+Len_period < len(d1)): \n",
    "                                    for l in range(index_k,index_k+Len_period):\n",
    "                                        mhap1.append(d1[l])\n",
    "                                        #mhap2.append(d2[l])\n",
    "                                        #mhap3.append(d3[l])\n",
    "    \n",
    "                                filter_lists[filter_index].append(mhap1)\n",
    "                                #filter_lists[filter_index].append(mhap2)\n",
    "                                #filter_lists[filter_index].append(mhap3)\n",
    "                                #add also what is time period\n",
    "                                filter_lists_index[filter_index].append(index_k)               \n",
    "                            index_k +=1\n",
    "\n",
    "            period_active.append(filter_lists)        \n",
    "            index_period_active.append(filter_lists_index)      \n",
    "        return period_active,index_period_active\n",
    "    \n",
    "    def get_graph_MHAP(self,activations,kernal_size,cluster_central):\n",
    "        kernal_size = [8,5,3]\n",
    "        #initilize the array of the output (size will be based on output class label)\n",
    "        #classes_lists = [[] for i in range(self.nb_classes)]    \n",
    "        graph1 = nx.DiGraph()\n",
    "        graph2 = nx.DiGraph()\n",
    "        graph3 = nx.DiGraph()\n",
    "        graph = nx.DiGraph()\n",
    "        graph_data_each_sample = []\n",
    "        node_assigned = []\n",
    "        #threshoulds = self.define_threshould_all_filters(activation_layers)\n",
    "        threshoulds = self.define_threshould(activation_layers)\n",
    "        #loop through each training sample\n",
    "        sample_cluster_mhap = []\n",
    "        for j in range(len(self.x_test)):\n",
    "            sample_x_graph=[]\n",
    "            graph_layer_node = [[] for i in range(len(kernal_size))]\n",
    "            index_layer = [[] for i in range(len(kernal_size))]\n",
    "            #loop through each layer in the network\n",
    "            # we need to take only the conv layers(not the conv and pooling)\n",
    "            #so we take each second index of the loop\n",
    "            activated_id = 1\n",
    "            #len of the period_will depend on the lyer kernal size (first will be same and then multily of previous)\n",
    "            Len_period = 8\n",
    "            filter_index = 0\n",
    "            layer_cluster_mhap = []\n",
    "            for l in range(0,self.netLayers+6):\n",
    "                flag = False\n",
    "                #get the id of the filter layer\n",
    "                if(l == 1):\n",
    "                    activated_id = 0\n",
    "                    Len_period = kernal_size[0]\n",
    "                    filter_index = 0\n",
    "                    flag = True                    \n",
    "                elif(l==4):\n",
    "                    activated_id = 1\n",
    "                    Len_period = kernal_size[0] * kernal_size[1]\n",
    "                    filter_index = 1\n",
    "                    flag = True  \n",
    "                elif(l==7):\n",
    "                    activated_id = 2\n",
    "                    Len_period = kernal_size[0]*kernal_size[1]*kernal_size[2]\n",
    "                    filter_index = 2\n",
    "                    flag = True\n",
    "                #so make sure that we only have the id of conv layer\n",
    "                if(flag):\n",
    "                    #here use networkx to create the directed graph      \n",
    "                    #active id is what conv layer of network, \n",
    "                    activated_nodes = activations[l]\n",
    "                    #each filter of the data(activated_nodes.shape[2]) has (n values based on strid(inour case =1))                       \n",
    "                    for i in range(0,activated_nodes.shape[2]):\n",
    "                        index_k = 0\n",
    "                        #loop throug the feature map\n",
    "                        prev_node1 = ''\n",
    "                        prev_node2 = ''\n",
    "                        prev_node3 = ''\n",
    "                        val = 0.8\n",
    "                        for k in (activated_nodes[j, :, i]):\n",
    "                            if(filter_index == 0):\n",
    "                                val = 0.07\n",
    "                            elif(filter_index==1):\n",
    "                                #print(threshoulds[filter_index][i][2].pdf(k))\n",
    "                                val = 0.15\n",
    "                            else:\n",
    "                                val = 0.05\n",
    "                                \n",
    "                            #if(k >= threshoulds[filter_index][4]):\n",
    "                            if(threshoulds[filter_index][i][2].pdf(k) >val):\n",
    "                                d1 =[]\n",
    "                                #d2 =[]\n",
    "                                #d3 =[]\n",
    "                                #get each dimention data\n",
    "                                for xe in(self.x_test[j]):\n",
    "                                    d1.append(xe[0])\n",
    "                                    #d2.append(xe[1])\n",
    "                                    #d3.append(xe[2])\n",
    "                                mhap1 = []\n",
    "                                #mhap2 = []\n",
    "                                #mhap3 = []\n",
    "                                if(index_k+Len_period < len(d1)): \n",
    "                                    for l in range(index_k,index_k+Len_period):\n",
    "                                        mhap1.append(d1[l])\n",
    "                                        #mhap2.append(d2[l])\n",
    "                                        #mhap3.append(d3[l])\n",
    "                                index_layer[activated_id].append(index_k)\n",
    "                                #here we have 3 MHAP we want to compre cluster\n",
    "                                cluster_1 = ''\n",
    "                                cluster_id_1 = ''\n",
    "                                #cluster_2 = ''\n",
    "                                #cluster_id_2 = ''\n",
    "                                #cluster_3 = ''\n",
    "                                #cluster_id_3 = ''\n",
    "                                if(len(mhap1)):\n",
    "                                    #print(mhap1)\n",
    "                                    #here we cheack cluster fit, we want build graph (sampleX1->)\n",
    "                                    #node_1,node2\n",
    "                                    x_n = []\n",
    "                                    x_n.append(mhap1)\n",
    "                                    cluster_id_1 = self.fitted_cluster(mhap1,cluster_central[filter_index])\n",
    "                                    #cluster_id_1 = cluster_central[filter_index].predict(x_n)\n",
    "                                    #cluster_id_1 = cluster_id_1[0] \n",
    "                                    #print(cluster_id_1)\n",
    "                                    #self.fitted_cluster(mhap1,cluster_central[filter_index])\n",
    "                                    cluster_1 = 'layer%s %s'%((activated_id),(cluster_id_1))\n",
    "                                    sample_x_graph.append(cluster_1)\n",
    "                                #if(len(mhap1)):\n",
    "                                #    cluster_id_2 = self.fitted_cluster(mhap2,cluster_central[filter_index])\n",
    "                                #    cluster_2 = 'Layer_%s %s'%(activated_id,cluster_id_2)\n",
    "                                #if(len(mhap1)):\n",
    "                                #    cluster_id_3 = self.fitted_cluster(mhap3,cluster_central[filter_index])\n",
    "                                #    cluster_3 = 'Layer_%s %s'%(activated_id,cluster_id_3)\n",
    "                                \n",
    "                                ################################################################# hooon\n",
    "                                layer_previous = []\n",
    "                                if(activated_id > 0 and (cluster_1 != '')):\n",
    "                                    for ju in range(index_k,index_k+Len_period):\n",
    "                                        index_layer_grap = 0 \n",
    "                                        for ku in index_layer[activated_id-1]:\n",
    "                                            if(ju == ku):\n",
    "                                                #means this is also mhap from previous layer\n",
    "                                                #print(graph_layer_node[activated_id-1][index_layer_grap])\n",
    "                                                if(index_layer_grap < len(graph_layer_node[activated_id-1])):\n",
    "                                                    layer_previous.append(graph_layer_node[activated_id-1][index_layer_grap])\n",
    "\n",
    "                                            index_layer_grap +=1\n",
    "                                    #print(layer_previous) \n",
    "                                if(cluster_1 == ''):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    graph_layer_node[activated_id].append(cluster_1)\n",
    "                                #sample_x_graph.append(cluster_1)\n",
    "                                if(activated_id == 0):\n",
    "                                    if(prev_node1 != '' and cluster_1 != ''):\n",
    "                                        #here g . add edges\n",
    "                                        if graph1.has_edge(prev_node1, cluster_1):\n",
    "                                        # we added this one before, just increase the weight by one\n",
    "                                            graph1[prev_node1][cluster_1]['weight'] += 1\n",
    "                                            graph[prev_node1][cluster_1]['weight'] += 1\n",
    "                                            \n",
    "                                        else:\n",
    "                                        # new edge. add with weight=1\n",
    "                                            if(prev_node1 != '' and cluster_1 != ''):\n",
    "                                                graph1.add_edge(prev_node1, cluster_1, weight=1)\n",
    "                                                graph.add_edge(prev_node1, cluster_1, weight=1)\n",
    "                                            #graph_layer_node[activated_id].append(cluster_1)\n",
    "                                        sample_x_graph.append(prev_node1)\n",
    "                                        sample_x_graph.append(cluster_1)\n",
    "                                #here we cheack if layer_previous not empty we add all its node to the current graph node\n",
    "                                #cluster_1\n",
    "                                elif(activated_id ==1):\n",
    "                                    #here we cheack if index mhap connect cluster id then we add edge\n",
    "                                    if(prev_node1 != '' and cluster_1 != ''):\n",
    "                                        if graph2.has_edge(prev_node1, cluster_1):\n",
    "                                        # we added this one before, just increase the weight by one\n",
    "                                            graph2[prev_node1][cluster_1]['weight'] += 1\n",
    "                                            graph[prev_node1][cluster_1]['weight'] += 1\n",
    "                                            if(layer_previous!=[]):\n",
    "                                                for k in layer_previous:\n",
    "                                                    if graph.has_edge(cluster_1,k):\n",
    "                                                        graph[cluster_1][k]['weight'] += 1\n",
    "                                                    else:\n",
    "                                                        if(prev_node1 != '' and cluster_1 != ''):\n",
    "                                                            graph.add_edge(cluster_1, k, weight=1)\n",
    "                                            #graph_layer_node[activated_id].append(cluster_1)\n",
    "                                        else:\n",
    "                                        # new edge. add with weight=1\n",
    "                                            graph2.add_edge(prev_node1, cluster_1, weight=1)\n",
    "                                            graph.add_edge(prev_node1, cluster_1, weight=1)\n",
    "                                            if(layer_previous!=[]):\n",
    "                                                for k in layer_previous:\n",
    "                                                    if graph.has_edge(cluster_1,k):\n",
    "                                                        graph[cluster_1][k]['weight'] += 1\n",
    "                                                    else:\n",
    "                                                        if(prev_node1 != '' and cluster_1 != ''):\n",
    "                                                            graph.add_edge(cluster_1, k, weight=1)\n",
    "                                            #graph_layer_node[activated_id].append(cluster_1)\n",
    "                                        sample_x_graph.append(prev_node1)\n",
    "                                        sample_x_graph.append(cluster_1)\n",
    "                                        if(layer_previous!=[]):\n",
    "                                            for k in layer_previous:\n",
    "                                                sample_x_graph.append(k)\n",
    "                                        \n",
    "                                else:\n",
    "                                    if(prev_node1 != '' and cluster_1 != ''):\n",
    "                                        if graph3.has_edge(prev_node1, cluster_1):\n",
    "                                        # we added this one before, just increase the weight by one\n",
    "                                            graph3[prev_node1][cluster_1]['weight'] += 1\n",
    "                                            graph[prev_node1][cluster_1]['weight'] += 1\n",
    "                                            #graph_layer_node[activated_id].append(cluster_1)\n",
    "                                            if(layer_previous!=[]):\n",
    "                                                for k in layer_previous:\n",
    "                                                    if graph.has_edge(cluster_1,k):\n",
    "                                                        graph[cluster_1][k]['weight'] += 1\n",
    "                                                    else:\n",
    "                                                        if(prev_node1 != '' and cluster_1 != ''):\n",
    "                                                            graph.add_edge(cluster_1, k, weight=1)\n",
    "                                        else:\n",
    "                                        # new edge. add with weight=1\n",
    "                                            graph3.add_edge(prev_node1, cluster_1, weight=1)\n",
    "                                            graph.add_edge(prev_node1, cluster_1, weight=1)\n",
    "                                            #graph_layer_node[activated_id].append(cluster_1)\n",
    "                                            if(layer_previous!=[]):\n",
    "                                                for k in layer_previous:\n",
    "                                                    if graph.has_edge(cluster_1,k):\n",
    "                                                        graph[cluster_1][k]['weight'] += 1\n",
    "                                                    else:\n",
    "                                                        if(prev_node1 != '' and cluster_1 != ''):\n",
    "                                                            graph.add_edge(cluster_1, k, weight=1)\n",
    "                                                        \n",
    "                                        sample_x_graph.append(prev_node1)\n",
    "                                        sample_x_graph.append(cluster_1)\n",
    "                                        if(layer_previous!=[]):\n",
    "                                            for k in layer_previous:\n",
    "                                                sample_x_graph.append(k)\n",
    "                                prev_node1 = 'layer%s %s'%((activated_id),(cluster_id_1))\n",
    "                                #prev_node2 = cluster_2\n",
    "                                #prev_node3 = cluster_3                                                           \n",
    "                            index_k +=1   \n",
    "                        #print('test_')\n",
    "            sample_cluster_mhap.append(sample_x_graph)\n",
    "        return graph,index_layer,sample_cluster_mhap\n",
    "\n",
    "    def normilization(self,data):\n",
    "        i = 0\n",
    "        datt = []\n",
    "        maxi = max(data)\n",
    "        mini = abs(min(data))\n",
    "        while (i< len(data)):\n",
    "            \n",
    "            if(data[i] >=0):\n",
    "                val = data[i]/maxi\n",
    "            else:\n",
    "                val = data[i]/mini\n",
    "         \n",
    "            datt.append(val)\n",
    "            i += 1\n",
    "            \n",
    "        return datt\n",
    "\n",
    "    \n",
    "    def fitted_cluster(self,data,cluster):\n",
    "        data = self.normilization(data)\n",
    "        cluster[0] = self.normilization(cluster[0])\n",
    "        mini =0\n",
    "        if(np.isnan(cluster[0]).any() == False & np.isinf(cluster[0]).any() == False):\n",
    "            mini = distance.euclidean(data,cluster[0])\n",
    "        cluster_id = 0\n",
    "        count = 0\n",
    "        for i in (cluster):\n",
    "            clu_nor = self.normilization(i)\n",
    "            if(np.isnan(clu_nor).any() == False & np.isinf(clu_nor).any() == False):\n",
    "                dist = distance.euclidean(data,clu_nor)\n",
    "                if(dist < mini):\n",
    "                    cluster_id = count\n",
    "                    mini = dist\n",
    "                count+=1\n",
    "        \n",
    "        return cluster_id\n",
    "    \n",
    "    def get_segmant_MHAP(self,activations,kernal_size,cluster_central,n,sgmant_lenth):\n",
    "        kernal_size = [8,5,3]\n",
    "        #initilize the array of the output (size will be based on output class label)\n",
    "        #classes_lists = [[] for i in range(self.nb_classes)]    \n",
    "        #threshoulds = self.define_threshould_all_filters(activation_layers)\n",
    "        threshoulds = self.define_threshould_all_filters(activation_layers)\n",
    "        #loop through each training sample\n",
    "        sample_cluster_mhap = []\n",
    "        sgmant_lenth = sgmant_lenth\n",
    "        start = 0\n",
    "        n = n\n",
    "        sgmant_lenth = sgmant_lenth\n",
    "        intervals = np.arange(start, sgmant_lenth * n , sgmant_lenth)\n",
    "        #print(intervals)\n",
    "        for j in range(len(self.x_test)):\n",
    "            graph_layer_node = [[] for i in range(len(kernal_size))]\n",
    "            index_layer = [[] for i in range(len(kernal_size))]\n",
    "            #print(j)\n",
    "            #    break\n",
    "            sample_x_graph=[]\n",
    "            segmant = [[] for i in range(n)]\n",
    "            interval = 0\n",
    "            #loop through each layer in the network\n",
    "            # we need to take only the conv layers(not the conv and pooling)\n",
    "            #so we take each second index of the loop\n",
    "            activated_id = 1\n",
    "            #len of the period_will depend on the lyer kernal size (first will be same and then multily of previous)\n",
    "            Len_period = 8\n",
    "            filter_index = 0\n",
    "            layer_cluster_mhap = []\n",
    "            for l in range(0,self.netLayers+6):\n",
    "                flag = False\n",
    "                #get the id of the filter layer\n",
    "                if(l == 1):\n",
    "                    activated_id = 0\n",
    "                    Len_period = kernal_size[0]\n",
    "                    filter_index = 0\n",
    "                    flag = True                    \n",
    "                elif(l==4):\n",
    "                    activated_id = 1\n",
    "                    Len_period = kernal_size[0] * kernal_size[1]\n",
    "                    filter_index = 1\n",
    "                    flag = True  \n",
    "                elif(l==7):\n",
    "                    activated_id = 2\n",
    "                    Len_period = kernal_size[0]*kernal_size[1]*kernal_size[2]\n",
    "                    filter_index = 2\n",
    "                    flag = True\n",
    "                #so make sure that we only have the id of conv layer\n",
    "                if(flag):\n",
    "                    #here use networkx to create the directed graph      \n",
    "                    #active id is what conv layer of network, \n",
    "                    activated_nodes = activations[l]\n",
    "                    #each filter of the data(activated_nodes.shape[2]) has (n values based on strid(inour case =1))                       \n",
    "                    for i in range(0,activated_nodes.shape[2]):\n",
    "                        index_k = 0\n",
    "                        #loop throug the feature map\n",
    "                        prev_node1 = ''\n",
    "                        prev_node2 = ''\n",
    "                        prev_node3 = ''\n",
    "                        val = 0.2\n",
    "                        for k in (activated_nodes[j, :, i]):\n",
    "                            if(filter_index == 0):\n",
    "                                val = 0.07\n",
    "                            elif(filter_index==1):\n",
    "                            #    #print(threshoulds[filter_index][i][2].pdf(k))\n",
    "                                val = 0.15\n",
    "                            else:\n",
    "                                val = 0.05\n",
    "                                \n",
    "                            if(k >= threshoulds[filter_index][3]):\n",
    "                            #if(threshoulds[filter_index][i][3].pdf(k) >val):\n",
    "                                #print('gg')\n",
    "                                d1 =[]\n",
    "                                d2 =[]\n",
    "                                #d3 =[]\n",
    "                                #get each dimention data\n",
    "                                for xe in(self.x_test[j]):\n",
    "                                    d1.append(xe[0])\n",
    "                                    d2.append(xe[1])\n",
    "                                    #d3.append(xe[2])\n",
    "                                mhap1 = []\n",
    "                                mhap2 = []\n",
    "                                #mhap3 = []\n",
    "                                if(index_k+Len_period < len(d1)): \n",
    "                                    for l in range(index_k,index_k+Len_period):\n",
    "                                        mhap1.append(d1[l])\n",
    "                                        mhap2.append(d2[l])\n",
    "                                        #mhap3.append(d3[l])\n",
    "                                index_layer[activated_id].append(index_k)\n",
    "                                #here we have 3 MHAP we want to compre cluster\n",
    "                                cluster_1 = ''\n",
    "                                cluster_id_1 = ''\n",
    "                                if(len(mhap1)):\n",
    "                                    #print(mhap1)\n",
    "                                    #here we cheack cluster fit, we want build graph (sampleX1->)\n",
    "                                    #node_1,node2\n",
    "                                    x_n = []\n",
    "                                    x_n.append(mhap1)\n",
    "                                    #print(len(cluster_central))\n",
    "                                    #if(len(cluster_central[filter_index])):\n",
    "                                    cluster_id_1 = self.fitted_cluster(mhap1,cluster_central[filter_index])\n",
    "                                    cluster_1 = 'layer%s %s'%((activated_id),(cluster_id_1))\n",
    "                                    if(index_k == 0):\n",
    "                                        segmant[0].append(cluster_1)\n",
    "                                    else:\n",
    "                                        index_segma = bisect.bisect_left(intervals, index_k)-1\n",
    "                                        #print()\n",
    "                                        #print(index_segma)\n",
    "                                        segmant[index_segma].append(cluster_1)\n",
    "                                    sample_x_graph.append(cluster_1)\n",
    "                                \n",
    "                                ################################################################# hooon\n",
    "                                layer_previous = []\n",
    "                                if(activated_id > 0 and (cluster_1 != '')):\n",
    "                                    for ju in range(index_k,index_k+Len_period):\n",
    "                                        index_layer_grap = 0 \n",
    "                                        for ku in index_layer[activated_id-1]:\n",
    "                                            if(ju == ku):\n",
    "                                                #means this is also mhap from previous layer\n",
    "                                                #print(graph_layer_node[activated_id-1][index_layer_grap])\n",
    "                                                if(index_layer_grap < len(graph_layer_node[activated_id-1])):\n",
    "                                                    layer_previous.append(graph_layer_node[activated_id-1][index_layer_grap])\n",
    "\n",
    "                                            index_layer_grap +=1\n",
    "                                    #print(layer_previous) \n",
    "                                if(cluster_1 == ''):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    graph_layer_node[activated_id].append(cluster_1)\n",
    "                                #sample_x_graph.append(cluster_1)\n",
    "                                if(activated_id == 0):\n",
    "                                    if(prev_node1 != '' and cluster_1 != ''):\n",
    "                                        sample_x_graph.append(prev_node1)\n",
    "                                        sample_x_graph.append(cluster_1)\n",
    "                                        if(index_k == 0):\n",
    "                                            segmant[0].append(prev_node1)\n",
    "                                            segmant[0].append(cluster_1)\n",
    "                                        else:      \n",
    "                                            index_segma = bisect.bisect_left(intervals, index_k)-1\n",
    "                                            #print(index_segma)\n",
    "                                            segmant[index_segma].append(prev_node1)\n",
    "                                            segmant[index_segma].append(cluster_1)\n",
    "                                #here we cheack if layer_previous not empty we add all its node to the current graph node\n",
    "                                #cluster_1\n",
    "                                elif(activated_id ==1):\n",
    "                                    #here we cheack if index mhap connect cluster id then we add edge\n",
    "                                    if(prev_node1 != '' and cluster_1 != ''):\n",
    "                                        sample_x_graph.append(prev_node1)\n",
    "                                        sample_x_graph.append(cluster_1)\n",
    "                                        if(index_k == 0):\n",
    "                                            segmant[0].append(prev_node1)\n",
    "                                            segmant[0].append(cluster_1)\n",
    "                                        else:      \n",
    "                                            index_segma = bisect.bisect_left(intervals, index_k)-1\n",
    "                                            #print(index_segma)\n",
    "                                            segmant[index_segma].append(prev_node1)\n",
    "                                            segmant[index_segma].append(cluster_1)\n",
    "                                        if(layer_previous!=[]):\n",
    "                                            for k in layer_previous:\n",
    "                                                sample_x_graph.append(k)\n",
    "                                                if(index_k == 0):\n",
    "                                                    segmant[0].append(k)\n",
    "                                                else:      \n",
    "                                                    index_segma = bisect.bisect_left(intervals, index_k)-1\n",
    "                                                    #print(index_segma)\n",
    "                                                    segmant[index_segma].append(k)\n",
    "                                        \n",
    "                                else:\n",
    "                                    if(prev_node1 != '' and cluster_1 != ''):                                                     \n",
    "                                        sample_x_graph.append(prev_node1)\n",
    "                                        sample_x_graph.append(cluster_1)\n",
    "                                        if(index_k == 0):\n",
    "                                            segmant[0].append(prev_node1)\n",
    "                                            segmant[0].append(cluster_1)\n",
    "                                        else:      \n",
    "                                            index_segma = bisect.bisect_left(intervals, index_k)-1\n",
    "                                            #print(index_segma)\n",
    "                                            segmant[index_segma].append(prev_node1)\n",
    "                                            segmant[index_segma].append(cluster_1)\n",
    "                                        if(layer_previous!=[]):\n",
    "                                            for k in layer_previous:\n",
    "                                                sample_x_graph.append(k)\n",
    "                                                if(index_k == 0):\n",
    "                                                    segmant[0].append(k)\n",
    "                                                else:      \n",
    "                                                    index_segma = bisect.bisect_left(intervals, index_k)-1\n",
    "                                                    #print(index_segma)\n",
    "                                                    segmant[index_segma].append(k)\n",
    "                                        \n",
    "                                prev_node1 = 'layer%s %s'%((activated_id),(cluster_id_1))\n",
    "                            index_k +=1   \n",
    "\n",
    "            sample_cluster_mhap.append(segmant)\n",
    "        return sample_cluster_mhap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "#session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 13 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visulization_traning = HighlyActivated(model,train_model,x_test,x_test,nb_classes,netLayers=3)\n",
    "activation_layers = visulization_traning.Activated_filters(example_id=1)\n",
    "#layer_a = visulization_traning.define_threshould(activation_layers,)\n",
    "period_active,index_period_active = visulization_traning.get_index_MHAP(activation_layers,kernal_size=[8,5,3])\n",
    "#x = visulization_traning.get_dimention_MHAP(x_validation)\n",
    "#sample_cluster_mhap = visulization_traning.get_data_mhap(activation_layers,[8,40,120],read_cluster)\n",
    "#layer_all = visulization_traning.define_threshould_all_filters(activation_layers,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Dec 11 11:43:54 2020\n",
    "\n",
    "@author: Raneen_new\n",
    "\"\"\"\n",
    "\n",
    "from matplotlib import pyplot\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.interpolate import interp1d\n",
    "from kneed import KneeLocator\n",
    "import numpy as np\n",
    "from kneed import KneeLocator\n",
    "from tslearn.clustering import KShape\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "import scipy.cluster.hierarchy as shc\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import scipy.cluster.hierarchy as hac\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import OPTICS\n",
    "\n",
    "class Clustering:\n",
    "    def __init__(self,cluster_lists):\n",
    "        self.cluster_lists = cluster_lists\n",
    "\n",
    "    def scale_data(self,cluster_lists):\n",
    "        i = 0\n",
    "        if(cluster_lists != []):\n",
    "            for data in cluster_lists:\n",
    "                #print(len(cluster_lists[i]))\n",
    "                for idx,seq in enumerate(data):\n",
    "                    if(seq != []):\n",
    "                        max_seq = max(seq)\n",
    "                        min_seq = min(seq)\n",
    "                        i = 0\n",
    "                        while (i < len(seq)):\n",
    "                            seq[i] = (seq[i] - min_seq) / (max_seq - min_seq)\n",
    "                            i += 1\n",
    "                    else:\n",
    "                        data.pop(idx)\n",
    "                i+=1        \n",
    "        return cluster_lists\n",
    "\n",
    "    def k_mean_clustering(self,num_clusters,data):\n",
    "        kmeans = KMeans(init=\"random\",n_clusters=num_clusters,n_init=10,max_iter=300,random_state=42)\n",
    "        kmeans.fit(data)\n",
    "        kmeans_kwargs = {\"init\": \"random\",\"n_init\": 12,\"max_iter\": 300,\"random_state\": 42,}\n",
    "        return kmeans.cluster_centers_\n",
    "\n",
    "    def K_shape_clustering(self,num_clusters,data,layer_len):\n",
    "        # Calculate length of maximal list\n",
    "        n = len(max(data, key=len))\n",
    "        # Make the lists equal in length\n",
    "        lst_2 = [x + [0]*(n-len(x)) for x in data]\n",
    "        a = np.nan_to_num(np.array(lst_2))\n",
    "        print('K_shape_data')\n",
    "        kshape = KShape(n_clusters=num_clusters, verbose=True, random_state=42)\n",
    "        kshape.fit(a)\n",
    "        name = 'MHAP_layer_data/cluster_center'+str(layer_len)+'.npy'\n",
    "        np.save(name,kshape.cluster_centers_)\n",
    "        return kshape.cluster_centers_\n",
    "    \n",
    "    def fancy_dendrogram(self,*args, **kwargs):\n",
    "        max_d = kwargs.pop('max_d', None)\n",
    "        if max_d and 'color_threshold' not in kwargs:\n",
    "            kwargs['color_threshold'] = max_d\n",
    "        annotate_above = kwargs.pop('annotate_above', 0)\n",
    "    \n",
    "        ddata = hac.dendrogram(*args, **kwargs)\n",
    "    \n",
    "        if not kwargs.get('no_plot', False):\n",
    "            pyplot.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "            pyplot.xlabel('sample index or (cluster size)')\n",
    "            pyplot.ylabel('distance')\n",
    "            for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "                x = 0.5 * sum(i[1:3])\n",
    "                y = d[1]\n",
    "                if y > annotate_above:\n",
    "                    pyplot.plot(x, y, 'o', c=c)\n",
    "                    pyplot.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                                 textcoords='offset points',\n",
    "                                 va='top', ha='center')\n",
    "            if max_d:\n",
    "                pyplot.axhline(y=max_d, c='k')\n",
    "        return ddata\n",
    "    \n",
    "    def print_clusters(self,timeSeries, Z, k, plot=False):\n",
    "        # k Number of clusters I'd like to extract\n",
    "        results = fcluster(Z, k, criterion='maxclust')\n",
    "    \n",
    "        # check the results\n",
    "        s = pd.Series(results)\n",
    "        clusters = s.unique()\n",
    "    \n",
    "        for c in clusters:\n",
    "            cluster_indeces = s[s==c].index\n",
    "            print(\"Cluster %d number of entries %d\" % (c, len(cluster_indeces)))\n",
    "            if plot:\n",
    "                timeSeries.T.iloc[:,cluster_indeces].plot()\n",
    "                pyplot.show()\n",
    "        return clusters\n",
    "    \n",
    "    def hierarchical_cluster(self,num_clusters,data):\n",
    "        #cluster = AgglomerativeClustering(n_clusters=num_clusters, affinity='euclidean', linkage='ward')  \n",
    "        #cluster.fit_predict(data)\n",
    "        # Do the clustering\n",
    "        data = np.array(data)\n",
    "        df = pd.DataFrame(data=data)\n",
    "        data = df.dropna()\n",
    "        Z = hac.linkage(data, method='complete', metric='euclidean')\n",
    "        \n",
    "        # Plot dendogram\n",
    "        pyplot.figure(figsize=(25, 10))\n",
    "        pyplot.title('Hierarchical Clustering Dendrogram')\n",
    "        pyplot.xlabel('sample index')\n",
    "        pyplot.ylabel('distance')\n",
    "        \"\"\"hac.dendrogram(\n",
    "            Z,\n",
    "            truncate_mode='lastp',  # show only the last p merged clusters\n",
    "            p=12,  # show only the last p merged clusters\n",
    "            show_leaf_counts=False,  # otherwise numbers in brackets are counts\n",
    "            leaf_rotation=90.,\n",
    "            leaf_font_size=12.,\n",
    "            show_contracted=True,  # to get a distribution impression in truncated branches\n",
    "        )\"\"\"\n",
    "        self.fancy_dendrogram(\n",
    "            Z,\n",
    "            truncate_mode='lastp',\n",
    "            p=12,\n",
    "            leaf_rotation=90.,\n",
    "            leaf_font_size=12.,\n",
    "            show_contracted=True,\n",
    "            annotate_above=10,  # useful in small plots so annotations don't overlap\n",
    "        )\n",
    "        pyplot.show()\n",
    "        return self.print_clusters(data, Z, num_clusters, plot=False)\n",
    "    \n",
    "    def DBscan_cluster(self,num_clusters,data):\n",
    "        data = np.array(data)\n",
    "        data[np.isnan(data)] = 0\n",
    "        clustering = DBSCAN(eps=0.8, min_samples = 15).fit(data)\n",
    "        return clustering\n",
    "    \n",
    "    def optic_cluster(self,num_clusters,data):\n",
    "        data = np.array(data)\n",
    "        data[np.isnan(data)] = 0\n",
    "        clustering = OPTICS(min_samples=num_clusters).fit(data)\n",
    "        return clustering\n",
    "    \n",
    "    def cluster_sequence_data(self,cluster_number,layer_len,cluser_data_pre_list1):\n",
    "        # scale the data between 0 and 1\n",
    "        cluster_centers = []\n",
    "        cluster_lists = cluser_data_pre_list1\n",
    "        \n",
    "        #loop thriugh the periods for each CNN layer\n",
    "        count = 0\n",
    "        for layer in (cluster_lists):\n",
    "            #cluster_centers.append(self.hierarchical_cluster(cluster_number[count],layer))\n",
    "            #cluster_centers.append(self.k_mean_clustering(cluster_number[count],layer))\n",
    "            cluster_centers.append(self.K_shape_clustering(cluster_number[count],layer,layer_len[count]))\n",
    "            #cluster_centers.append(self.DBscan_cluster(cluster_number[count],layer))\n",
    "            #cluster_centers.append(self.optic_cluster(cluster_number[count],layer))\n",
    "            count +=1\n",
    "        \n",
    "        return  cluster_centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now cluster the MHAP for each layer\n",
    "#put the data of each layer in one array [[l1],[l2],[l3]]\n",
    "cluser_data_pre_list = []\n",
    "filter_lists = [[] for i in range(3)]\n",
    "for i in range(len(period_active)):\n",
    "    for j in range(len(period_active[i])):\n",
    "        for k in range(len(period_active[i][j])):\n",
    "            filter_lists[j].append(period_active[i][j][k])\n",
    "\n",
    "cluser_data_pre_list.append([x for x in filter_lists[0] if x])\n",
    "cluser_data_pre_list.append([x for x in filter_lists[1] if x])\n",
    "cluser_data_pre_list.append([x for x in filter_lists[2] if x])\n",
    "print(len(cluser_data_pre_list[0]))\n",
    "print(len(cluser_data_pre_list[1]))\n",
    "print(len(cluser_data_pre_list[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_to_proportion(rows, proportion=1):\n",
    "        i = 0\n",
    "        new_data = []\n",
    "        new_data.append(rows[0])\n",
    "        k = 0\n",
    "        for i in (rows):\n",
    "            if(k == proportion):\n",
    "                new_data.append(i)\n",
    "                k = 0\n",
    "            k+=1\n",
    "        return new_data \n",
    "cluser_data_pre_list1 = []\n",
    "cluser_data_pre_list1.append(downsample_to_proportion(cluser_data_pre_list[0], 1000))\n",
    "cluser_data_pre_list1.append(downsample_to_proportion(cluser_data_pre_list[1], 1000))\n",
    "cluser_data_pre_list1.append(downsample_to_proportion(cluser_data_pre_list[2], 100))\n",
    "cluser_data_pre_list1 = np.array(cluser_data_pre_list1)\n",
    "\n",
    "clustering = Clustering(cluser_data_pre_list1)\n",
    "cluser_data_pre_list1 = clustering.scale_data(cluser_data_pre_list1)\n",
    "clustering = Clustering(cluser_data_pre_list1)\n",
    "cluster_central = clustering.cluster_sequence_data([35,25,15],[8,40,120],cluser_data_pre_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class Graph_embading:\n",
    "    def __init__(self,graph):\n",
    "        self.G = graph\n",
    "        \n",
    "    def save_graph(self,file_name):\n",
    "        #initialze Figure\n",
    "        graph = self.G\n",
    "        plt.figure(num=None, figsize=(20, 20))\n",
    "        plt.axis('off')\n",
    "        fig = plt.figure(1)\n",
    "        pos = nx.spring_layout(graph)\n",
    "        color_map = []\n",
    "        for node in G:\n",
    "            layer0 = \"Layer_0\"\n",
    "            layer1 = \"Layer_1\"\n",
    "            layer2 = \"Layer_2\"\n",
    "\n",
    "            if layer0 in node:\n",
    "                color_map.append('blue')\n",
    "            elif layer1 in node: \n",
    "                color_map.append('green') \n",
    "            else:\n",
    "                color_map.append('red') \n",
    "        nx.draw_networkx_nodes(graph,pos,node_color=color_map)\n",
    "        nx.draw_networkx_edges(graph,pos)\n",
    "        nx.draw_networkx_labels(graph,pos)\n",
    "\n",
    "        cut = 0.05\n",
    "        xmax = cut * max(xx for xx, yy in pos.values())\n",
    "        ymax = cut * max(yy for xx, yy in pos.values())\n",
    "        plt.xlim(0, xmax)\n",
    "        plt.ylim(0, ymax)\n",
    "\n",
    "        plt.savefig(file_name,bbox_inches=\"tight\")\n",
    "        pylab.show()\n",
    "        del fig\n",
    "\n",
    "        #Assuming that the graph g has nodes and edges entered\n",
    "        save_graph(g,\"my_graph.pdf\")\n",
    "        \n",
    "    def drwa_graph(self):\n",
    "        # use one of the edge properties to control line thickness\n",
    "        edgewidth = [ d['weight'] for (u,v,d) in self.G.edges(data=True)]\n",
    "        # layout\n",
    "        pos = nx.spring_layout(self.G)\n",
    "        color_map = []\n",
    "        for node in G:\n",
    "            layer0 = \"layer0\"\n",
    "            layer1 = \"layer1\"\n",
    "            layer2 = \"layer2\"\n",
    "\n",
    "            if layer0 in node:\n",
    "                color_map.append('blue')\n",
    "            elif layer1 in node: \n",
    "                color_map.append('green') \n",
    "            elif layer2 in node:\n",
    "                color_map.append('red') \n",
    "            else:\n",
    "                color_map.append('black')\n",
    "        nx.draw_networkx_nodes(self.G, pos,node_size=100,node_color=color_map)\n",
    "        nx.draw_networkx_edges(self.G, pos,)\n",
    "        #nx.draw_networkx_edges(self.G, pos, width=edgewidth,edge_color=edgewidth)\n",
    "        \n",
    "    def draw_layer_graph(self,graph):\n",
    "        G = graph\n",
    "        # use one of the edge properties to control line thickness\n",
    "        edgewidth = [ d['weight'] for (u,v,d) in G.edges(data=True)]\n",
    "        # layout\n",
    "        pos = nx.spring_layout(G)\n",
    "        nx.draw_networkx_nodes(G, pos,node_size=100)\n",
    "        nx.draw_networkx_edges(G, pos,)\n",
    "        #nx.draw_networkx_edges(G, pos, width=edgewidth,edge_color=edgewidth)\n",
    "        \n",
    "    def get_node_list(self):\n",
    "        nodes_list = np.array(list(self.G.nodes()))\n",
    "        node_name = nodes_list[:]\n",
    "        return node_name\n",
    "        \n",
    "    def get_rando_mwalk_node(self,node, path_length): \n",
    "        random_walk = [node]\n",
    "\n",
    "        for i in range(path_length-1):\n",
    "            temp = list(self.G.neighbors(node))\n",
    "            temp = list(set(temp) - set(random_walk))    \n",
    "            if len(temp) == 0:\n",
    "                break\n",
    "\n",
    "            random_node = random.choice(temp)\n",
    "            random_walk.append(random_node)\n",
    "            node = random_node\n",
    "\n",
    "        return random_walk\n",
    "    \n",
    "    def randome_walk_nodes(self,node_name):\n",
    "        # get list of all nodes from the graph\n",
    "        all_nodes = list(self.G.nodes())\n",
    "\n",
    "        random_walks = []\n",
    "        for n in (node_name):\n",
    "            for i in range(5):\n",
    "                random_walks.append(self.get_rando_mwalk_node(n,15))\n",
    "        # count of sequences\n",
    "        return random_walks\n",
    "    \n",
    "    def embed_graph(self,random_walks):\n",
    "        # train skip-gram (word2vec) model\n",
    "        model_w2v = Word2Vec(window = 4, sg = 1, hs = 0,\n",
    "                         negative = 10, # for negative sampling\n",
    "                         alpha=0.03, min_alpha=0.0007,\n",
    "                         seed = 14)\n",
    "\n",
    "        model_w2v.build_vocab(random_walks, progress_per=2)\n",
    "\n",
    "        model_w2v.train(random_walks, total_examples = model_w2v.corpus_count, epochs=20, report_delay=1)\n",
    "        return model_w2v\n",
    "    \n",
    "    def plot_embaded_graph(self,model,word_list):\n",
    "        X = model.wv[word_list]\n",
    "\n",
    "        # reduce dimensions to 2\n",
    "        pca = PCA(n_components=2)\n",
    "        result = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(12,9))\n",
    "        # create a scatter plot of the projection\n",
    "        plt.scatter(result[:, 0], result[:, 1])\n",
    "        for i, word in enumerate(word_list):\n",
    "            plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#G = nx.read_gpickle(\"graph_UWave.gpickle\")\n",
    "G,index,graph_data_sample = visulization_traning.get_graph_MHAP(activation_layers,[8,40,120],cluster_central)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(graph_data_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embaded = Graph_embading(G)\n",
    "graph_embaded.drwa_graph()\n",
    "node_names = graph_embaded.get_node_list()\n",
    "walks_nodes = graph_embaded.randome_walk_nodes(node_names)\n",
    "print(walks_nodes)\n",
    "embaded_graph = graph_embaded.embed_graph(walks_nodes)\n",
    "graph_embaded.plot_embaded_graph(embaded_graph,node_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have the graph, we empaded the nodes using deep walk\n",
    "#now we want to test the model by go through the data, devideing it into segment\n",
    "#for each segment we want to take its MHAP and then empade these nodes and added the empaded vector to array\n",
    "#at each sample we will have (n embaded vector (depends on size of segment)).\n",
    "#we take these n (order in time) and classifiy the ouput using LSTM, or GBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function of Time Series Embedding\n",
    "def timeseries_embedding(embedding_graph,node_names,timesereis_MHAP,number_seg):\n",
    "    feature_list = []\n",
    "    embed_vector = embaded_graph.wv[node_names]\n",
    "    for i,data in enumerate(timesereis_MHAP):\n",
    "        #compare the name with word_list and take its embedding\n",
    "        #loop through segmant\n",
    "        segmant = [[] for i in range(number_seg)]\n",
    "        #print(len(data))\n",
    "        for m,seg in enumerate(data):\n",
    "            temp = [0 for i in range(len(embed_vector[0]))]\n",
    "            #each seg has mhaps\n",
    "            for k,mhap in enumerate(seg):\n",
    "                for j,node in enumerate(node_names):\n",
    "                    if(mhap == node):\n",
    "                        temp += embed_vector[j]\n",
    "                        break\n",
    "            segmant[m].append(list(temp))\n",
    "        feature_list.append(segmant)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_cluster_mhap = visulization_traning.get_segmant_MHAP(activation_layers,[8,40,120],cluster_central,9,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('MHAP_layer_data/segmant_MHAP_.npy',sample_cluster_mhap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature = timeseries_embedding(graph_embaded,node_names,sample_cluster_mhap,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('MHAP_layer_data/new_traning_x.npy',new_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_feature = []\n",
    "for m,data in enumerate (new_feature):\n",
    "    segmant = []\n",
    "    for j,seg in enumerate(data):\n",
    "        segmant.append(seg[0])\n",
    "    x_train_feature.append(segmant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to convert the time series to 200*(15*100) as 2d to use xgboost)\n",
    "x_train_new = []\n",
    "for i, data in enumerate (x_train_feature):\n",
    "    seg = []\n",
    "    for j in (data):\n",
    "        for k in j:\n",
    "            seg.append(k)\n",
    "    x_train_new.append(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train =y_true\n",
    "#for i in y_true:\n",
    "#    y_train.append(np.where(i==1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGboost with 5 fold crosss validation\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "model = xgb.XGBClassifier()\n",
    "# evaluate the model\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, x_train_new, y_true, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_new,y_train\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_train_new, y_true, test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "evallist = [(dtrain, 'train')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 100\n",
    "param = {'max_depth': 5, 'eta': 1, 'objective': 'multi:softprob','num_class': nb_classes}\n",
    "bst = xgb.train(param, dtrain, num_round, evallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = bst.predict(dtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre =[]\n",
    "for i in ypred:\n",
    "    y_pre.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print())\n",
    "#print(y_training[0])\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test, y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
